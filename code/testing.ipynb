{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a17c0fea",
   "metadata": {},
   "source": [
    "## Load and clean the datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "46bcd565",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import neccessary libarries\n",
    "\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "import re\n",
    "from math import pi\n",
    "from typing import Iterable, Dict, Tuple, List\n",
    "import statsmodels.api as sm\n",
    "from __future__ import annotations\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "a03b6f5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load the data\n",
    "base_dir = Path('/Users/madalinaciolan/dev/explainable_ai')\n",
    "\n",
    "#load the energy demand data(load)\n",
    "load_path = base_dir / 'data' / 'Load' / 'load_history.csv'\n",
    "load_df = pd.read_csv(load_path)\n",
    "\n",
    "#load the temperature data\n",
    "temp_path = base_dir / 'data' / 'Load' / 'temperature_history.csv'\n",
    "temp_df = pd.read_csv(temp_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "acfe5dcc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   zone_id  year  month  day      h1      h2      h3      h4      h5      h6  \\\n",
      "1        1  2004      1    1  16,853  16,450  16,517  16,873  17,064  17,727   \n",
      "2        1  2004      1    2  14,155  14,038  14,019  14,489  14,920  16,072   \n",
      "3        1  2004      1    3  14,439  14,272  14,109  14,081  14,775  15,491   \n",
      "4        1  2004      1    4  11,273  10,415   9,943   9,859   9,881  10,248   \n",
      "5        1  2004      1    5  10,750  10,321  10,107  10,065  10,419  12,101   \n",
      "\n",
      "   ...     h15     h16     h17     h18     h19     h20     h21     h22  \\\n",
      "1  ...  13,518  13,138  14,130  16,809  18,150  18,235  17,925  16,904   \n",
      "2  ...  16,127  15,448  15,839  17,727  18,895  18,650  18,443  17,580   \n",
      "3  ...  13,507  13,414  13,826  15,825  16,996  16,394  15,406  14,278   \n",
      "4  ...  14,207  13,614  14,162  16,237  17,430  17,218  16,633  15,238   \n",
      "5  ...  13,845  14,350  15,501  17,307  18,786  19,089  19,192  18,416   \n",
      "\n",
      "      h23     h24  \n",
      "1  16,162  14,750  \n",
      "2  16,467  15,258  \n",
      "3  13,315  12,424  \n",
      "4  13,580  11,727  \n",
      "5  17,006  16,018  \n",
      "\n",
      "[5 rows x 28 columns]\n",
      "   station_id  year  month  day  h1  h2  h3  h4  h5  h6  ...   h15   h16  \\\n",
      "1           1  2004      1    1  46  46  45  41  39  35  ...  55.0  55.0   \n",
      "2           1  2004      1    2  43  44  46  46  47  47  ...  54.0  56.0   \n",
      "3           1  2004      1    3  45  46  46  44  43  46  ...  69.0  68.0   \n",
      "4           1  2004      1    4  63  62  62  62  60  60  ...  71.0  72.0   \n",
      "5           1  2004      1    5  64  63  65  64  64  64  ...  66.0  66.0   \n",
      "\n",
      "    h17   h18   h19   h20   h21   h22   h23   h24  \n",
      "1  52.0  46.0  40.0  40.0  39.0  38.0  40.0  41.0  \n",
      "2  57.0  53.0  50.0  47.0  46.0  45.0  45.0  45.0  \n",
      "3  68.0  65.0  64.0  63.0  62.0  63.0  63.0  62.0  \n",
      "4  71.0  68.0  67.0  67.0  65.0  64.0  65.0  64.0  \n",
      "5  66.0  66.0  63.0  54.0  52.0  49.0  47.0  47.0  \n",
      "\n",
      "[5 rows x 28 columns]\n"
     ]
    }
   ],
   "source": [
    "#fix the index for both dataframes\n",
    "load_df.index = range(1, len(load_df) + 1)\n",
    "print(load_df.head())\n",
    "\n",
    "temp_df.index = range(1, len(temp_df) + 1)\n",
    "print(temp_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "5fc20b43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 33000 entries, 1 to 33000\n",
      "Data columns (total 28 columns):\n",
      " #   Column   Non-Null Count  Dtype \n",
      "---  ------   --------------  ----- \n",
      " 0   zone_id  33000 non-null  int64 \n",
      " 1   year     33000 non-null  int64 \n",
      " 2   month    33000 non-null  int64 \n",
      " 3   day      33000 non-null  int64 \n",
      " 4   h1       31740 non-null  object\n",
      " 5   h2       31740 non-null  object\n",
      " 6   h3       31740 non-null  object\n",
      " 7   h4       31740 non-null  object\n",
      " 8   h5       31740 non-null  object\n",
      " 9   h6       31740 non-null  object\n",
      " 10  h7       31720 non-null  object\n",
      " 11  h8       31720 non-null  object\n",
      " 12  h9       31720 non-null  object\n",
      " 13  h10      31720 non-null  object\n",
      " 14  h11      31720 non-null  object\n",
      " 15  h12      31720 non-null  object\n",
      " 16  h13      31720 non-null  object\n",
      " 17  h14      31720 non-null  object\n",
      " 18  h15      31720 non-null  object\n",
      " 19  h16      31720 non-null  object\n",
      " 20  h17      31720 non-null  object\n",
      " 21  h18      31720 non-null  object\n",
      " 22  h19      31720 non-null  object\n",
      " 23  h20      31720 non-null  object\n",
      " 24  h21      31720 non-null  object\n",
      " 25  h22      31720 non-null  object\n",
      " 26  h23      31720 non-null  object\n",
      " 27  h24      31720 non-null  object\n",
      "dtypes: int64(4), object(24)\n",
      "memory usage: 7.0+ MB\n",
      "None\n",
      "zone_id       0\n",
      "year          0\n",
      "month         0\n",
      "day           0\n",
      "h1         1260\n",
      "h2         1260\n",
      "h3         1260\n",
      "h4         1260\n",
      "h5         1260\n",
      "h6         1260\n",
      "h7         1280\n",
      "h8         1280\n",
      "h9         1280\n",
      "h10        1280\n",
      "h11        1280\n",
      "h12        1280\n",
      "h13        1280\n",
      "h14        1280\n",
      "h15        1280\n",
      "h16        1280\n",
      "h17        1280\n",
      "h18        1280\n",
      "h19        1280\n",
      "h20        1280\n",
      "h21        1280\n",
      "h22        1280\n",
      "h23        1280\n",
      "h24        1280\n",
      "dtype: int64\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 18073 entries, 1 to 18073\n",
      "Data columns (total 28 columns):\n",
      " #   Column      Non-Null Count  Dtype  \n",
      "---  ------      --------------  -----  \n",
      " 0   station_id  18073 non-null  int64  \n",
      " 1   year        18073 non-null  int64  \n",
      " 2   month       18073 non-null  int64  \n",
      " 3   day         18073 non-null  int64  \n",
      " 4   h1          18073 non-null  int64  \n",
      " 5   h2          18073 non-null  int64  \n",
      " 6   h3          18073 non-null  int64  \n",
      " 7   h4          18073 non-null  int64  \n",
      " 8   h5          18073 non-null  int64  \n",
      " 9   h6          18073 non-null  int64  \n",
      " 10  h7          18062 non-null  float64\n",
      " 11  h8          18062 non-null  float64\n",
      " 12  h9          18062 non-null  float64\n",
      " 13  h10         18062 non-null  float64\n",
      " 14  h11         18062 non-null  float64\n",
      " 15  h12         18062 non-null  float64\n",
      " 16  h13         18062 non-null  float64\n",
      " 17  h14         18062 non-null  float64\n",
      " 18  h15         18062 non-null  float64\n",
      " 19  h16         18062 non-null  float64\n",
      " 20  h17         18062 non-null  float64\n",
      " 21  h18         18062 non-null  float64\n",
      " 22  h19         18062 non-null  float64\n",
      " 23  h20         18062 non-null  float64\n",
      " 24  h21         18062 non-null  float64\n",
      " 25  h22         18062 non-null  float64\n",
      " 26  h23         18062 non-null  float64\n",
      " 27  h24         18062 non-null  float64\n",
      "dtypes: float64(18), int64(10)\n",
      "memory usage: 3.9 MB\n",
      "None\n",
      "station_id     0\n",
      "year           0\n",
      "month          0\n",
      "day            0\n",
      "h1             0\n",
      "h2             0\n",
      "h3             0\n",
      "h4             0\n",
      "h5             0\n",
      "h6             0\n",
      "h7            11\n",
      "h8            11\n",
      "h9            11\n",
      "h10           11\n",
      "h11           11\n",
      "h12           11\n",
      "h13           11\n",
      "h14           11\n",
      "h15           11\n",
      "h16           11\n",
      "h17           11\n",
      "h18           11\n",
      "h19           11\n",
      "h20           11\n",
      "h21           11\n",
      "h22           11\n",
      "h23           11\n",
      "h24           11\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "#check the info and missing values for both dataframes\n",
    "print(load_df.info())\n",
    "print(load_df.isna().sum())\n",
    "\n",
    "print(temp_df.info())\n",
    "print(temp_df.isna().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "4fe29248",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zone_id      int64\n",
      "year         int64\n",
      "month        int64\n",
      "day          int64\n",
      "h1         float64\n",
      "h2         float64\n",
      "h3         float64\n",
      "h4         float64\n",
      "h5         float64\n",
      "h6         float64\n",
      "h7         float64\n",
      "h8         float64\n",
      "h9         float64\n",
      "h10        float64\n",
      "h11        float64\n",
      "h12        float64\n",
      "h13        float64\n",
      "h14        float64\n",
      "h15        float64\n",
      "h16        float64\n",
      "h17        float64\n",
      "h18        float64\n",
      "h19        float64\n",
      "h20        float64\n",
      "h21        float64\n",
      "h22        float64\n",
      "h23        float64\n",
      "h24        float64\n",
      "dtype: object\n",
      "zone_id       0\n",
      "year          0\n",
      "month         0\n",
      "day           0\n",
      "h1         1260\n",
      "h2         1260\n",
      "h3         1260\n",
      "h4         1260\n",
      "h5         1260\n",
      "h6         1260\n",
      "h7         1280\n",
      "h8         1280\n",
      "h9         1280\n",
      "h10        1280\n",
      "h11        1280\n",
      "h12        1280\n",
      "h13        1280\n",
      "h14        1280\n",
      "h15        1280\n",
      "h16        1280\n",
      "h17        1280\n",
      "h18        1280\n",
      "h19        1280\n",
      "h20        1280\n",
      "h21        1280\n",
      "h22        1280\n",
      "h23        1280\n",
      "h24        1280\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# ---- Clean the load dataframe ----   \n",
    "load_df_clean = load_df.copy()\n",
    "cols = [f\"h{i}\" for i in range(1, 25)]\n",
    "for col in cols:\n",
    "    load_df_clean[col] = load_df_clean[col].str.replace(\",\", \"\").astype(float)\n",
    "\n",
    "print(load_df_clean.dtypes)\n",
    "print(load_df_clean.isna().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "791a3b79",
   "metadata": {},
   "source": [
    "## Fill in missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "86fed1df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Remaining NaNs per column after interpolation:  h1     0\n",
      "h2     0\n",
      "h3     0\n",
      "h4     0\n",
      "h5     0\n",
      "h6     0\n",
      "h7     0\n",
      "h8     0\n",
      "h9     0\n",
      "h10    0\n",
      "h11    0\n",
      "h12    0\n",
      "h13    0\n",
      "h14    0\n",
      "h15    0\n",
      "h16    0\n",
      "h17    0\n",
      "h18    0\n",
      "h19    0\n",
      "h20    0\n",
      "h21    0\n",
      "h22    0\n",
      "h23    0\n",
      "h24    0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# --- Fill NaNs in load_df_clean using interpolation before/after ---\n",
    "# 1) Horizontal interpolation within each day across h1..h24\n",
    "#    (fills gaps inside a day's 24 hours using surrounding hours)\n",
    "load_df_clean[cols] = load_df_clean[cols].interpolate(method='linear', axis=1, limit_direction='both')\n",
    "\n",
    "# 2) Vertical interpolation across days for each hour h1..h24\n",
    "#    (fills gaps across days using surrounding days for each hour)\n",
    "load_df_clean[cols] = load_df_clean[cols].interpolate(method='linear',  axis=0, limit_direction='both')\n",
    "\n",
    "# check if all NaNs in the load data frame have been filled\n",
    "print(\"\\nRemaining NaNs per column after interpolation: \", load_df_clean[cols].isna().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "eac8d82f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zone_id      int64\n",
      "year         int64\n",
      "month        int64\n",
      "day          int64\n",
      "h1         float64\n",
      "h2         float64\n",
      "h3         float64\n",
      "h4         float64\n",
      "h5         float64\n",
      "h6         float64\n",
      "h7         float64\n",
      "h8         float64\n",
      "h9         float64\n",
      "h10        float64\n",
      "h11        float64\n",
      "h12        float64\n",
      "h13        float64\n",
      "h14        float64\n",
      "h15        float64\n",
      "h16        float64\n",
      "h17        float64\n",
      "h18        float64\n",
      "h19        float64\n",
      "h20        float64\n",
      "h21        float64\n",
      "h22        float64\n",
      "h23        float64\n",
      "h24        float64\n",
      "dtype: object\n",
      "station_id     0\n",
      "year           0\n",
      "month          0\n",
      "day            0\n",
      "h1             0\n",
      "h2             0\n",
      "h3             0\n",
      "h4             0\n",
      "h5             0\n",
      "h6             0\n",
      "h7            11\n",
      "h8            11\n",
      "h9            11\n",
      "h10           11\n",
      "h11           11\n",
      "h12           11\n",
      "h13           11\n",
      "h14           11\n",
      "h15           11\n",
      "h16           11\n",
      "h17           11\n",
      "h18           11\n",
      "h19           11\n",
      "h20           11\n",
      "h21           11\n",
      "h22           11\n",
      "h23           11\n",
      "h24           11\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# ---- Clean the temperature dataframe ----  \n",
    "print(load_df_clean.dtypes) #no need to change types, they are already integers and floats\n",
    "print(temp_df.isna().sum()) #NaNs only in temperature colums h7 -> h24\n",
    "temp_df_clean = temp_df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "af9ff0e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Fill NaNs in load_df_clean using interpolation before/after ---\n",
    "#  Vertical interpolation across days for each hour h7..h24\n",
    "#    (fills gaps across days using surrounding days for each hour)\n",
    "cols_temp = [f\"h{i}\" for i in range(7, 25)]\n",
    "for col in cols_temp:\n",
    "    temp_df_clean[col] = temp_df_clean[col].interpolate(method='linear', axis=0, limit_direction='both')    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "2114585e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Remaining NaNs per column after interpolation:  station_id    0\n",
      "year          0\n",
      "month         0\n",
      "day           0\n",
      "h1            0\n",
      "h2            0\n",
      "h3            0\n",
      "h4            0\n",
      "h5            0\n",
      "h6            0\n",
      "h7            0\n",
      "h8            0\n",
      "h9            0\n",
      "h10           0\n",
      "h11           0\n",
      "h12           0\n",
      "h13           0\n",
      "h14           0\n",
      "h15           0\n",
      "h16           0\n",
      "h17           0\n",
      "h18           0\n",
      "h19           0\n",
      "h20           0\n",
      "h21           0\n",
      "h22           0\n",
      "h23           0\n",
      "h24           0\n",
      "dtype: int64\n",
      "   zone_id  year  month  day       h1       h2       h3       h4       h5  \\\n",
      "1        1  2004      1    1  16853.0  16450.0  16517.0  16873.0  17064.0   \n",
      "2        1  2004      1    2  14155.0  14038.0  14019.0  14489.0  14920.0   \n",
      "3        1  2004      1    3  14439.0  14272.0  14109.0  14081.0  14775.0   \n",
      "4        1  2004      1    4  11273.0  10415.0   9943.0   9859.0   9881.0   \n",
      "5        1  2004      1    5  10750.0  10321.0  10107.0  10065.0  10419.0   \n",
      "\n",
      "        h6  ...      h15      h16      h17      h18      h19      h20  \\\n",
      "1  17727.0  ...  13518.0  13138.0  14130.0  16809.0  18150.0  18235.0   \n",
      "2  16072.0  ...  16127.0  15448.0  15839.0  17727.0  18895.0  18650.0   \n",
      "3  15491.0  ...  13507.0  13414.0  13826.0  15825.0  16996.0  16394.0   \n",
      "4  10248.0  ...  14207.0  13614.0  14162.0  16237.0  17430.0  17218.0   \n",
      "5  12101.0  ...  13845.0  14350.0  15501.0  17307.0  18786.0  19089.0   \n",
      "\n",
      "       h21      h22      h23      h24  \n",
      "1  17925.0  16904.0  16162.0  14750.0  \n",
      "2  18443.0  17580.0  16467.0  15258.0  \n",
      "3  15406.0  14278.0  13315.0  12424.0  \n",
      "4  16633.0  15238.0  13580.0  11727.0  \n",
      "5  19192.0  18416.0  17006.0  16018.0  \n",
      "\n",
      "[5 rows x 28 columns]\n"
     ]
    }
   ],
   "source": [
    "#check if all NaNs in the temperature data frame have been filled\n",
    "print(\"\\nRemaining NaNs per column after interpolation: \", temp_df_clean.isna().sum()) \n",
    "\n",
    "print(load_df_clean.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03ba4a64",
   "metadata": {},
   "source": [
    "## Reshape both data frames from wide to long format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "9c3c7b41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   zone_id           date_time     load\n",
      "0        1 2004-01-01 00:00:00  16853.0\n",
      "1        1 2004-01-01 01:00:00  16450.0\n",
      "2        1 2004-01-01 02:00:00  16517.0\n",
      "3        1 2004-01-01 03:00:00  16873.0\n",
      "4        1 2004-01-01 04:00:00  17064.0\n"
     ]
    }
   ],
   "source": [
    "# ----Reshaping the load dataframe from wide to long format ----\n",
    "\n",
    "# Melt the dataframe to have a long format with columns: year, month, day, hour, load\n",
    "hours_cols = [f\"h{i}\" for i in range(1, 25)]\n",
    "load_long = load_df_clean.melt(\n",
    "    id_vars=[\"zone_id\", \"year\", \"month\", \"day\"],\n",
    "    value_vars=hours_cols,\n",
    "    var_name=\"hour\",\n",
    "    value_name=\"load\",\n",
    ")\n",
    "\n",
    "# Convert hour column to numeric hour\n",
    "load_long['hour'] = load_long['hour'].str.extract('h(\\d+)').astype(int)\n",
    "\n",
    "# Create a datetime column\n",
    "load_long['date_time'] = pd.to_datetime(load_long[['year', 'month', 'day']]) + pd.to_timedelta(load_long['hour'] - 1, unit='h')\n",
    "\n",
    "#Keep only relevant columns and sort by date\n",
    "load_long = load_long[['zone_id','date_time', 'load']].sort_values(['zone_id', 'date_time']).reset_index(drop=True)\n",
    "print(load_long.head())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "9121858d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   station_id  year  month  day  h1  h2  h3  h4  h5  h6  ...   h15   h16  \\\n",
      "1           1  2004      1    1  46  46  45  41  39  35  ...  55.0  55.0   \n",
      "2           1  2004      1    2  43  44  46  46  47  47  ...  54.0  56.0   \n",
      "3           1  2004      1    3  45  46  46  44  43  46  ...  69.0  68.0   \n",
      "4           1  2004      1    4  63  62  62  62  60  60  ...  71.0  72.0   \n",
      "5           1  2004      1    5  64  63  65  64  64  64  ...  66.0  66.0   \n",
      "\n",
      "    h17   h18   h19   h20   h21   h22   h23   h24  \n",
      "1  52.0  46.0  40.0  40.0  39.0  38.0  40.0  41.0  \n",
      "2  57.0  53.0  50.0  47.0  46.0  45.0  45.0  45.0  \n",
      "3  68.0  65.0  64.0  63.0  62.0  63.0  63.0  62.0  \n",
      "4  71.0  68.0  67.0  67.0  65.0  64.0  65.0  64.0  \n",
      "5  66.0  66.0  63.0  54.0  52.0  49.0  47.0  47.0  \n",
      "\n",
      "[5 rows x 28 columns]\n",
      "   station_id  year  month  day  hour  temperature  date_time\n",
      "0           1  2004      1    1     1         46.0 2004-01-01\n",
      "1           1  2004      1    2     1         43.0 2004-01-02\n",
      "2           1  2004      1    3     1         45.0 2004-01-03\n",
      "3           1  2004      1    4     1         63.0 2004-01-04\n",
      "4           1  2004      1    5     1         64.0 2004-01-05\n"
     ]
    }
   ],
   "source": [
    "print(temp_df_clean.head())\n",
    "\n",
    "# ----Reshaping the temperature dataframe from wide to long format ----\n",
    "\n",
    "# Melt the dataframe to have a long format with columns: year, month, day, hour, temperature\n",
    "temp_hour_cols = [f\"h{i}\" for i in range(1, 25)]\n",
    "temp_long = temp_df_clean.melt(\n",
    "    id_vars=[\"station_id\", \"year\", \"month\", \"day\"],\n",
    "    value_vars=temp_hour_cols,\n",
    "    var_name=\"hour\",\n",
    "    value_name=\"temperature\",\n",
    ")\n",
    "\n",
    "# Convert hour column to numeric hour\n",
    "temp_long['hour'] = temp_long['hour'].str.extract('h(\\d+)').astype(int)\n",
    "\n",
    "# Create a datetime column\n",
    "temp_long['date_time'] = pd.to_datetime(temp_long[['year', 'month', 'day']]) + pd.to_timedelta(temp_long['hour'] - 1, unit='h')\n",
    "\n",
    "#Keep only relevant columns and sort by date\n",
    "#temp_long = temp_long[['station_id','date_time', 'temperature']].sort_values(by='date_time').reset_index(drop=True)\n",
    "#print(temp_long.head())\n",
    "\n",
    "# Pivot so each station is one column\n",
    "temp_wide = temp_long.pivot(index='date_time', columns='station_id', values='temperature')\n",
    "temp_wide.columns = [f\"station_{col}\" for col in temp_wide.columns]\n",
    "\n",
    "print(temp_long.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e062b92c",
   "metadata": {},
   "source": [
    "## Merge the dataframes \n",
    "### Two options: attach all of the stations' temperature to each load record or infer a map of correlations.\n",
    "### Have chosen the first option as it is recommended for modeling. I have merged the load data with all stations' temperatures so each load observation has temp_station_1, ... , temp_station_11 as features. This avoids the need to have a  zone <-> station map."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "c643b94b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        zone_id           date_time     load  station_1  station_2  station_3  \\\n",
      "0             1 2004-01-01 00:00:00  16853.0       46.0       38.0       44.0   \n",
      "1             1 2004-01-01 01:00:00  16450.0       46.0       36.0       42.0   \n",
      "2             1 2004-01-01 02:00:00  16517.0       45.0       35.0       40.0   \n",
      "3             1 2004-01-01 03:00:00  16873.0       41.0       30.0       36.0   \n",
      "4             1 2004-01-01 04:00:00  17064.0       39.0       30.0       34.0   \n",
      "...         ...                 ...      ...        ...        ...        ...   \n",
      "791995       20 2008-07-07 19:00:00  74728.0        NaN        NaN        NaN   \n",
      "791996       20 2008-07-07 20:00:00  74728.0        NaN        NaN        NaN   \n",
      "791997       20 2008-07-07 21:00:00  74728.0        NaN        NaN        NaN   \n",
      "791998       20 2008-07-07 22:00:00  74728.0        NaN        NaN        NaN   \n",
      "791999       20 2008-07-07 23:00:00  74728.0        NaN        NaN        NaN   \n",
      "\n",
      "        station_4  station_5  station_6  station_7  station_8  station_9  \\\n",
      "0            45.0       42.0       44.0       45.0       43.0       41.0   \n",
      "1            43.0       42.0       43.0       44.0       44.0       39.0   \n",
      "2            41.0       40.0       42.0       41.0       42.0       36.0   \n",
      "3            37.0       39.0       38.0       40.0       34.0       35.0   \n",
      "4            33.0       40.0       38.0       35.0       30.0       33.0   \n",
      "...           ...        ...        ...        ...        ...        ...   \n",
      "791995        NaN        NaN        NaN        NaN        NaN        NaN   \n",
      "791996        NaN        NaN        NaN        NaN        NaN        NaN   \n",
      "791997        NaN        NaN        NaN        NaN        NaN        NaN   \n",
      "791998        NaN        NaN        NaN        NaN        NaN        NaN   \n",
      "791999        NaN        NaN        NaN        NaN        NaN        NaN   \n",
      "\n",
      "        station_10  station_11  \n",
      "0             42.0        36.0  \n",
      "1             43.0        32.0  \n",
      "2             43.0        31.0  \n",
      "3             39.0        30.0  \n",
      "4             35.0        34.0  \n",
      "...            ...         ...  \n",
      "791995         NaN         NaN  \n",
      "791996         NaN         NaN  \n",
      "791997         NaN         NaN  \n",
      "791998         NaN         NaN  \n",
      "791999         NaN         NaN  \n",
      "\n",
      "[792000 rows x 14 columns]\n"
     ]
    }
   ],
   "source": [
    "# --- Merge: attach all stations' temperatures to each load record ---\n",
    "merged_all = load_long.merge(temp_wide.reset_index(), on='date_time', how='left')\n",
    "print(merged_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "1df76c24",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "zone_id          0\n",
       "date_time        0\n",
       "load             0\n",
       "station_1     3360\n",
       "station_2     3360\n",
       "station_3     3360\n",
       "station_4     3360\n",
       "station_5     3360\n",
       "station_6     3360\n",
       "station_7     3360\n",
       "station_8     3360\n",
       "station_9     3360\n",
       "station_10    3360\n",
       "station_11    3360\n",
       "dtype: int64"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_all.isnull().sum() #check for missing values after the merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "6c7efa26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        zone_id           date_time     load  station_1  station_2  station_3  \\\n",
      "0             1 2004-01-01 00:00:00  16853.0       46.0       38.0       44.0   \n",
      "1             1 2004-01-01 01:00:00  16450.0       46.0       36.0       42.0   \n",
      "2             1 2004-01-01 02:00:00  16517.0       45.0       35.0       40.0   \n",
      "3             1 2004-01-01 03:00:00  16873.0       41.0       30.0       36.0   \n",
      "4             1 2004-01-01 04:00:00  17064.0       39.0       30.0       34.0   \n",
      "...         ...                 ...      ...        ...        ...        ...   \n",
      "788635       20 2008-06-30 19:00:00  74728.0       67.0       54.0       63.5   \n",
      "788636       20 2008-06-30 20:00:00  74728.0       65.5       51.0       61.5   \n",
      "788637       20 2008-06-30 21:00:00  74728.0       64.5       48.5       58.0   \n",
      "788638       20 2008-06-30 22:00:00  74728.0       61.0       48.5       59.0   \n",
      "788639       20 2008-06-30 23:00:00  74728.0       60.0       48.5       59.0   \n",
      "\n",
      "        station_4  station_5  station_6  station_7  station_8  station_9  \\\n",
      "0            45.0       42.0       44.0       45.0       43.0       41.0   \n",
      "1            43.0       42.0       43.0       44.0       44.0       39.0   \n",
      "2            41.0       40.0       42.0       41.0       42.0       36.0   \n",
      "3            37.0       39.0       38.0       40.0       34.0       35.0   \n",
      "4            33.0       40.0       38.0       35.0       30.0       33.0   \n",
      "...           ...        ...        ...        ...        ...        ...   \n",
      "788635       64.0       65.0       57.5       61.0       63.0       62.5   \n",
      "788636       56.5       64.0       56.0       57.5       55.5       62.5   \n",
      "788637       54.0       62.5       55.0       56.5       55.5       60.0   \n",
      "788638       51.0       61.0       56.0       59.0       55.0       60.0   \n",
      "788639       52.0       60.0       56.0       59.5       55.0       61.0   \n",
      "\n",
      "        station_10  station_11  \n",
      "0             42.0        36.0  \n",
      "1             43.0        32.0  \n",
      "2             43.0        31.0  \n",
      "3             39.0        30.0  \n",
      "4             35.0        34.0  \n",
      "...            ...         ...  \n",
      "788635        63.0        78.0  \n",
      "788636        60.0        73.0  \n",
      "788637        59.5        72.0  \n",
      "788638        58.0        70.0  \n",
      "788639        56.0        70.0  \n",
      "\n",
      "[788640 rows x 14 columns]\n"
     ]
    }
   ],
   "source": [
    "merged_all_cleaned = merged_all.dropna().reset_index(drop=True) #drop rows with any NaNs and reset index\n",
    "print(merged_all_cleaned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "f4c911f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "zone_id       0\n",
       "date_time     0\n",
       "load          0\n",
       "station_1     0\n",
       "station_2     0\n",
       "station_3     0\n",
       "station_4     0\n",
       "station_5     0\n",
       "station_6     0\n",
       "station_7     0\n",
       "station_8     0\n",
       "station_9     0\n",
       "station_10    0\n",
       "station_11    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_all_cleaned.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eed306b",
   "metadata": {},
   "source": [
    "## Build time features:\n",
    "####    •\tcyclical encodings (hour/dow/doy)\n",
    "####\t•\tweekend + US holiday flags (holiday, day-before, day-after)\n",
    "####\t•\tapproximate sunrise/sunset + daylight flags (no lat/lon as the geografical zone is unknown) \n",
    "####\t•\tHeating Degree Days(HDD) / Cooling Degree Days (CDD) from all temp_station_* columns (auto-detects temps) with a threshold of 18 °C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "b699b9f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "                            Unnamed: 0                 2004  \\\n",
      "0                       New Year's Day  Thursday, January 1   \n",
      "1  Birthday of Martin Luther King, Jr.   Monday, January 19   \n",
      "2                Washington's Birthday  Monday, February 16   \n",
      "3                         Memorial Day       Monday, May 31   \n",
      "4                     Independence Day       Monday, July 5   \n",
      "\n",
      "                        2005                 2006                 2007  \\\n",
      "0  Friday, December 31, 2004    Monday, January 2    Monday, January 1   \n",
      "1         Monday, January 17   Monday, January 16   Monday, January 15   \n",
      "2        Monday, February 21  Monday, February 20  Monday, February 19   \n",
      "3             Monday, May 30       Monday, May 29       Monday, May 28   \n",
      "4             Monday, July 4      Tuesday, July 4    Wednesday, July 4   \n",
      "\n",
      "                  2008  \n",
      "0   Tuesday, January 1  \n",
      "1   Monday, January 21  \n",
      "2  Monday, February 18  \n",
      "3       Monday, May 26  \n",
      "4       Friday, July 4  \n"
     ]
    }
   ],
   "source": [
    "holidays = pd.read_csv(base_dir / 'data' / 'Load' / 'Holiday_List.csv')\n",
    "print(type(holidays))\n",
    "print(holidays.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "c81af8d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   zone_id           date_time     load  station_1  station_2  station_3  \\\n",
      "0        1 2004-01-01 00:00:00  16853.0       46.0       38.0       44.0   \n",
      "1        1 2004-01-01 01:00:00  16450.0       46.0       36.0       42.0   \n",
      "2        1 2004-01-01 02:00:00  16517.0       45.0       35.0       40.0   \n",
      "3        1 2004-01-01 03:00:00  16873.0       41.0       30.0       36.0   \n",
      "4        1 2004-01-01 04:00:00  17064.0       39.0       30.0       34.0   \n",
      "\n",
      "   station_4  station_5  station_6  station_7  ...  is_day_before_holiday  \\\n",
      "0       45.0       42.0       44.0       45.0  ...                      0   \n",
      "1       43.0       42.0       43.0       44.0  ...                      0   \n",
      "2       41.0       40.0       42.0       41.0  ...                      0   \n",
      "3       37.0       39.0       38.0       40.0  ...                      0   \n",
      "4       33.0       40.0       38.0       35.0  ...                      0   \n",
      "\n",
      "   is_day_after_holiday  sunrise_hour_approx  sunset_hour_approx  \\\n",
      "0                     0             5.000221           20.499705   \n",
      "1                     0             5.000221           20.499705   \n",
      "2                     0             5.000221           20.499705   \n",
      "3                     0             5.000221           20.499705   \n",
      "4                     0             5.000221           20.499705   \n",
      "\n",
      "   daylight_hours_approx  is_daylight_approx  daylight_proxy  temp_mean  HDD  \\\n",
      "0              15.499484                   0        0.017166  42.363636  0.0   \n",
      "1              15.499484                   0        0.017166  41.272727  0.0   \n",
      "2              15.499484                   0        0.017166  39.636364  0.0   \n",
      "3              15.499484                   0        0.017166  36.272727  0.0   \n",
      "4              15.499484                   0        0.017166  34.636364  0.0   \n",
      "\n",
      "         CDD  \n",
      "0  24.363636  \n",
      "1  23.272727  \n",
      "2  21.636364  \n",
      "3  18.272727  \n",
      "4  16.636364  \n",
      "\n",
      "[5 rows x 34 columns]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# --- Robust: convert wide holiday matrix (rows=holiday names, cols=years) into tidy 'date' list\n",
    "def _normalize_holiday_matrix(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Handles cases where the holiday name is in the index or in the first column,\n",
    "    and drops any non-year columns like 'Unnamed: 0' before parsing.\n",
    "    \"\"\"\n",
    "    # If holiday names are the index, promote to a column named 'holiday'\n",
    "    if df.index.name is None or df.index.equals(pd.RangeIndex(len(df))):\n",
    "        # Try to detect when the first column is the holiday name\n",
    "        first_col = df.columns[0]\n",
    "        # If the first column looks like year (numeric), then names must be in the index -> add from index\n",
    "        if pd.to_numeric(pd.Index(df.columns), errors=\"coerce\").notna().sum() > 0 and not pd.api.types.is_object_dtype(df[first_col]):\n",
    "            df2 = df.rename_axis(\"holiday\").reset_index()\n",
    "        else:\n",
    "            # Assume first column holds holiday names\n",
    "            df2 = df.copy()\n",
    "            if first_col.lower() != \"holiday\":\n",
    "                df2 = df2.rename(columns={first_col: \"holiday\"})\n",
    "    else:\n",
    "        df2 = df.rename_axis(\"holiday\").reset_index()\n",
    "\n",
    "    # Melt to long format\n",
    "    long = df2.melt(id_vars=[\"holiday\"], var_name=\"year\", value_name=\"raw\")\n",
    "\n",
    "    # Keep only rows where 'year' is numeric (drop 'Unnamed: 0' etc.)\n",
    "    long[\"year_num\"] = pd.to_numeric(long[\"year\"], errors=\"coerce\")\n",
    "    long = long[long[\"year_num\"].notna()].copy()\n",
    "\n",
    "    # Clean raw strings; drop blanks\n",
    "    long[\"raw\"] = long[\"raw\"].astype(str).str.strip()\n",
    "    long = long[~long[\"raw\"].isin([\"\", \"nan\", \"NaN\"])]\n",
    "\n",
    "    # Parse each cell: if text already contains a 4-digit year, parse as-is; else append the numeric year\n",
    "    def _parse_row(row):\n",
    "        txt = row[\"raw\"].strip().strip('\"').strip(\"'\")\n",
    "        yr  = int(row[\"year_num\"])\n",
    "        txt = re.sub(r\"\\s+,\", \",\", txt)          # fix stray spaces before commas\n",
    "        txt = re.sub(r\"\\s{2,}\", \" \", txt)        # collapse multiple spaces\n",
    "        if re.search(r\"\\b\\d{4}\\b\", txt):\n",
    "            dt = pd.to_datetime(txt, errors=\"coerce\")\n",
    "        else:\n",
    "            dt = pd.to_datetime(f\"{txt}, {yr}\", errors=\"coerce\")\n",
    "        return dt\n",
    "\n",
    "    long[\"date\"] = long.apply(_parse_row, axis=1)\n",
    "    dates = (long[\"date\"]\n",
    "             .dropna()\n",
    "             .dt.normalize()\n",
    "             .drop_duplicates()\n",
    "             .sort_values())\n",
    "    return pd.DataFrame({\"date\": dates})\n",
    "\n",
    "\n",
    "def make_features(\n",
    "    df: pd.DataFrame,\n",
    "    holidays,                     # can be a path or a wide-format DataFrame\n",
    "    *,\n",
    "    date_col: str = \"date_time\",\n",
    "    temp_prefix: str = \"temp_station_\",\n",
    "    base_c: float = 18.0,       # base temperature for HDD/CDD calculations (°C)\n",
    "    doy_period: int = 366,      # leap-year friendly\n",
    "    sr_mean: float = 6.5,       # 06:30 average sunrise time\n",
    "    sr_amp: float = 1.5,        # +/- 1.5 hours amplitude (earliest sunrise ~05:00, latest ~08:00) \n",
    "    ss_mean: float = 18.5,      # 18:30 average sunset time\n",
    "    ss_amp: float = 2.0         # +/- 2.0 hours amplitude (earliest sunset ~17:30, latest ~20:30)\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Adds:\n",
    "      • cyclical encodings (hour/dow/doy)\n",
    "      • weekend + holiday flags\n",
    "      • approximate sunrise/sunset + daylight\n",
    "      • HDD/CDD from all temp_station_* columns (°C)\n",
    "\n",
    "    Parameters:\n",
    "    - df: Input dataframe with a datetime column and temperature columns\n",
    "    - holidays: DataFrame containing US holidays with a 'date' column\n",
    "    - date_col: Name of the datetime column in df\n",
    "    - temp_prefix: Prefix for temperature columns to consider for HDD/CDD calculations\n",
    "    - base_c: Base temperature in Celsius for HDD/CDD calculations\n",
    "    - doy_period: Period for cyclical encoding of day of year (366 to include leap year)\n",
    "    - sr_mean, sr_amp: Mean and amplitude for sunrise time approximation\n",
    "    - ss_mean, ss_amp: Mean and amplitude for sunset time approximation\n",
    "    \"\"\"\n",
    "    out = df.copy()\n",
    "\n",
    "    # ---- timestamp\n",
    "    out[date_col] = pd.to_datetime(out[date_col], errors=\"coerce\")\n",
    "    if out[date_col].isna().any():\n",
    "        raise ValueError(f\"{date_col} contains NaT after parsing.\")\n",
    "    dt = out[date_col]\n",
    "\n",
    "    out[\"hour\"] = dt.dt.hour\n",
    "    out[\"dow\"] = dt.dt.dayofweek\n",
    "    out[\"is_weekend\"] = (out[\"dow\"] >= 5).astype(int)\n",
    "    doy = dt.dt.dayofyear.astype(int)\n",
    "\n",
    "    # ---- cyclical encodings\n",
    "    out[\"hour_sin\"] = np.sin(2 * pi * out[\"hour\"] / 24)\n",
    "    out[\"hour_cos\"] = np.cos(2 * pi * out[\"hour\"] / 24)\n",
    "    out[\"dow_sin\"]  = np.sin(2 * pi * out[\"dow\"] / 7)\n",
    "    out[\"dow_cos\"]  = np.cos(2 * pi * out[\"dow\"] / 7)\n",
    "    out[\"doy_sin\"]  = np.sin(2 * pi * doy / doy_period)\n",
    "    out[\"doy_cos\"]  = np.cos(2 * pi * doy / doy_period)\n",
    "\n",
    "    # ---- holidays (accept path or DataFrame)\n",
    "    if isinstance(holidays, (str, os.PathLike)):\n",
    "        holidays_df = pd.read_csv(holidays)\n",
    "    elif isinstance(holidays, pd.DataFrame):\n",
    "        holidays_df = holidays.copy()\n",
    "    else:\n",
    "        raise ValueError(\"holidays must be a CSV path or a DataFrame\")\n",
    "\n",
    "    # normalize the wide holiday matrix into a single 'date' column\n",
    "    holidays_norm = _normalize_holiday_matrix(holidays_df)\n",
    "    hol_days = set(holidays_norm[\"date\"].unique())\n",
    "    cal_day = dt.dt.normalize()\n",
    "\n",
    "    out[\"is_holiday\"]            = cal_day.isin(hol_days).astype(int)\n",
    "    out[\"is_day_before_holiday\"] = cal_day.isin({d - pd.Timedelta(days=1) for d in hol_days}).astype(int)\n",
    "    out[\"is_day_after_holiday\"]  = cal_day.isin({d + pd.Timedelta(days=1) for d in hol_days}).astype(int)\n",
    "\n",
    "    # sunrise/sunset-like approximation\n",
    "    sunrise_hour = sr_mean - sr_amp * np.cos(2 * pi * doy / doy_period)\n",
    "    sunset_hour  = ss_mean + ss_amp * np.cos(2 * pi * doy / doy_period)\n",
    "\n",
    "    out[\"sunrise_hour_approx\"]   = sunrise_hour\n",
    "    out[\"sunset_hour_approx\"]    = sunset_hour\n",
    "    out[\"daylight_hours_approx\"] = sunset_hour - sunrise_hour\n",
    "    out[\"is_daylight_approx\"]    = ((out[\"hour\"] >= sunrise_hour.astype(int)) &\n",
    "                                    (out[\"hour\"] <  sunset_hour.astype(int))).astype(int)\n",
    "    out[\"daylight_proxy\"]        = np.sin(2 * pi * doy / doy_period)\n",
    "\n",
    "    # ---- HDD/CDD from temperature columns (°C)\n",
    "    temp_cols = [c for c in out.columns if c.startswith(temp_prefix)]\n",
    "    if not temp_cols:\n",
    "        # fallback: any column containing 'temp'\n",
    "        temp_cols = [c for c in out.columns if re.search(r\"temp\", c, re.IGNORECASE)]\n",
    "    if temp_cols:\n",
    "        out[\"temp_mean\"] = out[temp_cols].mean(axis=1)\n",
    "        out[\"HDD\"] = (base_c - out[\"temp_mean\"]).clip(lower=0)\n",
    "        out[\"CDD\"] = (out[\"temp_mean\"] - base_c).clip(lower=0)\n",
    "\n",
    "    return out\n",
    "\n",
    "features_df = make_features(merged_all_cleaned, holidays, temp_prefix='station_')\n",
    "print(features_df.head())     "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e58d0832",
   "metadata": {},
   "source": [
    "## Create lag/rolling features\n",
    "#### The lag/rolling features are created only for exogenous variables -columns that are not the target(target is load) and are not identifiers (zone_id, date_time)- used later in an ARIMAX model (ARIMA with exogenous regressors). \n",
    "##### Examples: temperature (station_1,..., station_11, temp_mean), derived weather indicators (HDD, CDD), holidays(is_day_before_holiday, is_day_after_holiday), sunlight (sunrise_hour_approx, sunset_hour_approx, daylight_hours_approx, is_daylight_approx, daylight_proxy)\n",
    "##### Electricity load is strongly influenced by weather and holidays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "df5ad649",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def add_exogenous_lags_for_features_df(\n",
    "    df: pd.DataFrame,\n",
    "    *,\n",
    "    group_col: str = \"zone_id\",\n",
    "    date_col: str = \"date_time\",\n",
    "    # temperature-like columns\n",
    "    temp_prefix: str = \"station_\",                              # station_1..station_11\n",
    "    include_extra_temp_cols: Iterable[str] = (\"temp_mean\", \"HDD\", \"CDD\"),\n",
    "    temp_lags: tuple[int, ...] = (1, 3, 6, 24, 48, 168),        # lags in hours\n",
    "    temp_rolls: tuple[int, ...] = (6, 24, 168),                 # rolling windows in hours\n",
    "    # holiday columns present in features_df\n",
    "    holiday_cols: Iterable[str] = (\"is_day_before_holiday\", \"is_day_after_holiday\"),\n",
    "    holiday_lags: tuple[int, ...] = (),                         # usually not needed\n",
    "    holiday_leads: tuple[int, ...] = (24, 168)                  # usually only 1-day lead\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Adds lagged and rolling-window features for exogenous variables (non-target, non-ID columns)\n",
    "    to be used in an ARIMAX model (ARIMA with exogenous regressors).\n",
    "\n",
    "    Parameters:\n",
    "    - df: Input dataframe with a datetime column and temperature columns\n",
    "    - group_col: Column to group by (e.g., zone_id) for separate lagging per group\n",
    "    - date_col: Name of the datetime column in df\n",
    "    - temp_prefix: Prefix for temperature columns to consider for lagging/rolling\n",
    "    - include_extra_temp_cols: Additional temperature-like columns to include (e.g., temp_mean, HDD, CDD)\n",
    "    - temp_lags: Tuple of integer lags (in hours) to create for temperature-like columns\n",
    "    - temp_rolls: Tuple of integer rolling window sizes (in hours) to create for temperature-like columns\n",
    "    - holidays_cols: List of holiday-related binary columns to consider for lagging/leading\n",
    "    - holiday_lags: Tuple of integer lags (in hours) to create for holiday columns\n",
    "    - holiday_leads: Tuple of integer leads (in hours) to create for holiday columns\n",
    "\n",
    "    Returns:\n",
    "    - DataFrame with original and new lagged/rolling features\n",
    "    \"\"\"\n",
    "    out = df.copy()\n",
    "    out = out.sort_values([group_col, date_col], kind=\"mergesort\")\n",
    "    out[date_col] = pd.to_datetime(out[date_col], errors=\"coerce\")\n",
    "\n",
    "    # ---- collect temperature columns\n",
    "    temp_cols = [c for c in out.columns if c.startswith(temp_prefix)]\n",
    "    for c in include_extra_temp_cols:\n",
    "        if c in out.columns:\n",
    "            temp_cols.append(c)\n",
    "    temp_cols = list(dict.fromkeys(temp_cols))  # de-duplicate while keeping order\n",
    "\n",
    "    if not temp_cols:\n",
    "        raise ValueError(\"No temperature columns found (station_* / temp_mean / HDD / CDD).\")\n",
    "\n",
    "    # ---- group per zone\n",
    "    g = out.groupby(group_col, group_keys=False)\n",
    "\n",
    "    # ---- temperature lags\n",
    "    for L in temp_lags:\n",
    "        for c in temp_cols:\n",
    "            out[f\"{c}_lag{L}\"] = g[c].shift(L)\n",
    "\n",
    "    # ---- temperature rolling means\n",
    "    for W in temp_rolls:\n",
    "        for c in temp_cols:\n",
    "            out[f\"{c}_roll{W}\"] = g[c].transform(lambda s: s.rolling(W, min_periods=1).mean())\n",
    "\n",
    "    # ---- holiday lags/leads\n",
    "    hol_cols_present = [h for h in holiday_cols if h in out.columns]\n",
    "\n",
    "    for L in holiday_lags:\n",
    "        for h in hol_cols_present:\n",
    "            out[f\"{h}_lag{L}\"] = g[h].shift(L)\n",
    "\n",
    "    for H in holiday_leads:\n",
    "        for h in hol_cols_present:\n",
    "            out[f\"{h}_lead{H}\"] = g[h].shift(-H)\n",
    "\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "8cf4e473",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/y6/w4x154ys7zd4n1hm8b7sn2hh0000gp/T/ipykernel_2518/1059531882.py:60: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  out[f\"{c}_roll{W}\"] = g[c].transform(lambda s: s.rolling(W, min_periods=1).mean())\n",
      "/var/folders/y6/w4x154ys7zd4n1hm8b7sn2hh0000gp/T/ipykernel_2518/1059531882.py:60: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  out[f\"{c}_roll{W}\"] = g[c].transform(lambda s: s.rolling(W, min_periods=1).mean())\n",
      "/var/folders/y6/w4x154ys7zd4n1hm8b7sn2hh0000gp/T/ipykernel_2518/1059531882.py:60: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  out[f\"{c}_roll{W}\"] = g[c].transform(lambda s: s.rolling(W, min_periods=1).mean())\n",
      "/var/folders/y6/w4x154ys7zd4n1hm8b7sn2hh0000gp/T/ipykernel_2518/1059531882.py:60: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  out[f\"{c}_roll{W}\"] = g[c].transform(lambda s: s.rolling(W, min_periods=1).mean())\n",
      "/var/folders/y6/w4x154ys7zd4n1hm8b7sn2hh0000gp/T/ipykernel_2518/1059531882.py:60: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  out[f\"{c}_roll{W}\"] = g[c].transform(lambda s: s.rolling(W, min_periods=1).mean())\n",
      "/var/folders/y6/w4x154ys7zd4n1hm8b7sn2hh0000gp/T/ipykernel_2518/1059531882.py:60: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  out[f\"{c}_roll{W}\"] = g[c].transform(lambda s: s.rolling(W, min_periods=1).mean())\n",
      "/var/folders/y6/w4x154ys7zd4n1hm8b7sn2hh0000gp/T/ipykernel_2518/1059531882.py:60: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  out[f\"{c}_roll{W}\"] = g[c].transform(lambda s: s.rolling(W, min_periods=1).mean())\n",
      "/var/folders/y6/w4x154ys7zd4n1hm8b7sn2hh0000gp/T/ipykernel_2518/1059531882.py:60: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  out[f\"{c}_roll{W}\"] = g[c].transform(lambda s: s.rolling(W, min_periods=1).mean())\n",
      "/var/folders/y6/w4x154ys7zd4n1hm8b7sn2hh0000gp/T/ipykernel_2518/1059531882.py:60: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  out[f\"{c}_roll{W}\"] = g[c].transform(lambda s: s.rolling(W, min_periods=1).mean())\n",
      "/var/folders/y6/w4x154ys7zd4n1hm8b7sn2hh0000gp/T/ipykernel_2518/1059531882.py:60: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  out[f\"{c}_roll{W}\"] = g[c].transform(lambda s: s.rolling(W, min_periods=1).mean())\n",
      "/var/folders/y6/w4x154ys7zd4n1hm8b7sn2hh0000gp/T/ipykernel_2518/1059531882.py:60: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  out[f\"{c}_roll{W}\"] = g[c].transform(lambda s: s.rolling(W, min_periods=1).mean())\n",
      "/var/folders/y6/w4x154ys7zd4n1hm8b7sn2hh0000gp/T/ipykernel_2518/1059531882.py:60: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  out[f\"{c}_roll{W}\"] = g[c].transform(lambda s: s.rolling(W, min_periods=1).mean())\n",
      "/var/folders/y6/w4x154ys7zd4n1hm8b7sn2hh0000gp/T/ipykernel_2518/1059531882.py:60: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  out[f\"{c}_roll{W}\"] = g[c].transform(lambda s: s.rolling(W, min_periods=1).mean())\n",
      "/var/folders/y6/w4x154ys7zd4n1hm8b7sn2hh0000gp/T/ipykernel_2518/1059531882.py:60: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  out[f\"{c}_roll{W}\"] = g[c].transform(lambda s: s.rolling(W, min_periods=1).mean())\n",
      "/var/folders/y6/w4x154ys7zd4n1hm8b7sn2hh0000gp/T/ipykernel_2518/1059531882.py:60: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  out[f\"{c}_roll{W}\"] = g[c].transform(lambda s: s.rolling(W, min_periods=1).mean())\n",
      "/var/folders/y6/w4x154ys7zd4n1hm8b7sn2hh0000gp/T/ipykernel_2518/1059531882.py:60: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  out[f\"{c}_roll{W}\"] = g[c].transform(lambda s: s.rolling(W, min_periods=1).mean())\n",
      "/var/folders/y6/w4x154ys7zd4n1hm8b7sn2hh0000gp/T/ipykernel_2518/1059531882.py:60: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  out[f\"{c}_roll{W}\"] = g[c].transform(lambda s: s.rolling(W, min_periods=1).mean())\n",
      "/var/folders/y6/w4x154ys7zd4n1hm8b7sn2hh0000gp/T/ipykernel_2518/1059531882.py:60: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  out[f\"{c}_roll{W}\"] = g[c].transform(lambda s: s.rolling(W, min_periods=1).mean())\n",
      "/var/folders/y6/w4x154ys7zd4n1hm8b7sn2hh0000gp/T/ipykernel_2518/1059531882.py:60: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  out[f\"{c}_roll{W}\"] = g[c].transform(lambda s: s.rolling(W, min_periods=1).mean())\n",
      "/var/folders/y6/w4x154ys7zd4n1hm8b7sn2hh0000gp/T/ipykernel_2518/1059531882.py:60: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  out[f\"{c}_roll{W}\"] = g[c].transform(lambda s: s.rolling(W, min_periods=1).mean())\n",
      "/var/folders/y6/w4x154ys7zd4n1hm8b7sn2hh0000gp/T/ipykernel_2518/1059531882.py:60: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  out[f\"{c}_roll{W}\"] = g[c].transform(lambda s: s.rolling(W, min_periods=1).mean())\n",
      "/var/folders/y6/w4x154ys7zd4n1hm8b7sn2hh0000gp/T/ipykernel_2518/1059531882.py:60: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  out[f\"{c}_roll{W}\"] = g[c].transform(lambda s: s.rolling(W, min_periods=1).mean())\n",
      "/var/folders/y6/w4x154ys7zd4n1hm8b7sn2hh0000gp/T/ipykernel_2518/1059531882.py:60: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  out[f\"{c}_roll{W}\"] = g[c].transform(lambda s: s.rolling(W, min_periods=1).mean())\n",
      "/var/folders/y6/w4x154ys7zd4n1hm8b7sn2hh0000gp/T/ipykernel_2518/1059531882.py:60: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  out[f\"{c}_roll{W}\"] = g[c].transform(lambda s: s.rolling(W, min_periods=1).mean())\n",
      "/var/folders/y6/w4x154ys7zd4n1hm8b7sn2hh0000gp/T/ipykernel_2518/1059531882.py:60: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  out[f\"{c}_roll{W}\"] = g[c].transform(lambda s: s.rolling(W, min_periods=1).mean())\n",
      "/var/folders/y6/w4x154ys7zd4n1hm8b7sn2hh0000gp/T/ipykernel_2518/1059531882.py:60: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  out[f\"{c}_roll{W}\"] = g[c].transform(lambda s: s.rolling(W, min_periods=1).mean())\n",
      "/var/folders/y6/w4x154ys7zd4n1hm8b7sn2hh0000gp/T/ipykernel_2518/1059531882.py:60: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  out[f\"{c}_roll{W}\"] = g[c].transform(lambda s: s.rolling(W, min_periods=1).mean())\n",
      "/var/folders/y6/w4x154ys7zd4n1hm8b7sn2hh0000gp/T/ipykernel_2518/1059531882.py:60: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  out[f\"{c}_roll{W}\"] = g[c].transform(lambda s: s.rolling(W, min_periods=1).mean())\n",
      "/var/folders/y6/w4x154ys7zd4n1hm8b7sn2hh0000gp/T/ipykernel_2518/1059531882.py:60: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  out[f\"{c}_roll{W}\"] = g[c].transform(lambda s: s.rolling(W, min_periods=1).mean())\n",
      "/var/folders/y6/w4x154ys7zd4n1hm8b7sn2hh0000gp/T/ipykernel_2518/1059531882.py:60: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  out[f\"{c}_roll{W}\"] = g[c].transform(lambda s: s.rolling(W, min_periods=1).mean())\n",
      "/var/folders/y6/w4x154ys7zd4n1hm8b7sn2hh0000gp/T/ipykernel_2518/1059531882.py:71: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  out[f\"{h}_lead{H}\"] = g[h].shift(-H)\n",
      "/var/folders/y6/w4x154ys7zd4n1hm8b7sn2hh0000gp/T/ipykernel_2518/1059531882.py:71: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  out[f\"{h}_lead{H}\"] = g[h].shift(-H)\n",
      "/var/folders/y6/w4x154ys7zd4n1hm8b7sn2hh0000gp/T/ipykernel_2518/1059531882.py:71: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  out[f\"{h}_lead{H}\"] = g[h].shift(-H)\n",
      "/var/folders/y6/w4x154ys7zd4n1hm8b7sn2hh0000gp/T/ipykernel_2518/1059531882.py:71: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  out[f\"{h}_lead{H}\"] = g[h].shift(-H)\n"
     ]
    }
   ],
   "source": [
    "features_with_lags = add_exogenous_lags_for_features_df(\n",
    "    features_df,                    # my DataFrame\n",
    "    group_col=\"zone_id\",\n",
    "    date_col=\"date_time\",\n",
    "    temp_prefix=\"station_\",         # station_1..station_11\n",
    "    include_extra_temp_cols=(\"temp_mean\", \"HDD\", \"CDD\"),\n",
    "    temp_lags=(1, 3, 6, 24, 48, 168),\n",
    "    temp_rolls=(6, 24, 168),\n",
    "    holiday_cols=(\"is_day_before_holiday\", \"is_day_after_holiday\"),\n",
    "    holiday_lags=(),                # usually leave empty\n",
    "    holiday_leads=(24, 168)         # holidays are known → safe leads\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "127ccfa9",
   "metadata": {},
   "source": [
    "### Check again for data quality: no missing timestamps and ensure consistent hourly intervals. \n",
    "#### Each zone_id must have a continous, clean and hourly time series with aligned features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "2a30c733",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   zone_id           date_time     load  station_1  station_2  station_3  \\\n",
      "0        1 2004-01-01 00:00:00  16853.0       46.0       38.0       44.0   \n",
      "1        1 2004-01-01 01:00:00  16450.0       46.0       36.0       42.0   \n",
      "2        1 2004-01-01 02:00:00  16517.0       45.0       35.0       40.0   \n",
      "3        1 2004-01-01 03:00:00  16873.0       41.0       30.0       36.0   \n",
      "4        1 2004-01-01 04:00:00  17064.0       39.0       30.0       34.0   \n",
      "\n",
      "   station_4  station_5  station_6  station_7  ...  station_9_roll168  \\\n",
      "0       45.0       42.0       44.0       45.0  ...          41.000000   \n",
      "1       43.0       42.0       43.0       44.0  ...          40.000000   \n",
      "2       41.0       40.0       42.0       41.0  ...          38.666667   \n",
      "3       37.0       39.0       38.0       40.0  ...          37.750000   \n",
      "4       33.0       40.0       38.0       35.0  ...          36.800000   \n",
      "\n",
      "   station_10_roll168  station_11_roll168  temp_mean_roll168  HDD_roll168  \\\n",
      "0           42.000000               36.00          42.363636          0.0   \n",
      "1           42.500000               34.00          41.818182          0.0   \n",
      "2           42.666667               33.00          41.090909          0.0   \n",
      "3           41.750000               32.25          39.886364          0.0   \n",
      "4           40.400000               32.60          38.836364          0.0   \n",
      "\n",
      "   CDD_roll168  is_day_before_holiday_lead24  is_day_after_holiday_lead24  \\\n",
      "0    24.363636                           0.0                          1.0   \n",
      "1    23.818182                           0.0                          1.0   \n",
      "2    23.090909                           0.0                          1.0   \n",
      "3    21.886364                           0.0                          1.0   \n",
      "4    20.836364                           0.0                          1.0   \n",
      "\n",
      "   is_day_before_holiday_lead168  is_day_after_holiday_lead168  \n",
      "0                            0.0                           0.0  \n",
      "1                            0.0                           0.0  \n",
      "2                            0.0                           0.0  \n",
      "3                            0.0                           0.0  \n",
      "4                            0.0                           0.0  \n",
      "\n",
      "[5 rows x 164 columns]\n",
      "date_time\n",
      "0 days 01:00:00    788620\n",
      "Name: count, dtype: int64\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "# Sort and check continuity per zone_id\n",
    "features_with_lags = features_with_lags.sort_values([\"zone_id\", \"date_time\"]).reset_index(drop=True)\n",
    "\n",
    "print(features_with_lags.head())\n",
    "\n",
    "# Check time step gaps per zone_id\n",
    "gaps = (\n",
    "    features_with_lags.groupby(\"zone_id\")[\"date_time\"]          #handle multiple zones separately\n",
    "    .diff()                                                     #computes the time difference between consecutive timestamps within each zone\n",
    "    .dropna()                                                   #removes the first row in each group(since it has no previous timestamp to diff against)\n",
    "    .value_counts()                                             #counts how many times each time gap occurs across the dataset\n",
    ")\n",
    "print(gaps.head())\n",
    "print(gaps.isna().sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "a867a0fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "is_day_after_holiday_lead168    3360\n",
       "station_9_lag168                3360\n",
       "station_1_lag168                3360\n",
       "station_2_lag168                3360\n",
       "station_3_lag168                3360\n",
       "station_4_lag168                3360\n",
       "station_5_lag168                3360\n",
       "station_6_lag168                3360\n",
       "station_8_lag168                3360\n",
       "station_7_lag168                3360\n",
       "dtype: int64"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check for missing values \n",
    "features_with_lags.isna().sum().sort_values(ascending=False).head(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "fadf8cce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zone_id             0\n",
      "CDD_lag48           0\n",
      "station_2_lag168    0\n",
      "station_3_lag168    0\n",
      "station_4_lag168    0\n",
      "station_5_lag168    0\n",
      "station_6_lag168    0\n",
      "station_7_lag168    0\n",
      "station_8_lag168    0\n",
      "station_9_lag168    0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Lagged and lead features introduced some NaNs at the start/end of each zone_id group:\n",
    "- when shifting the time series backward by 168 hrs(7 days), the first 168 rows of each zone_id cannot be filled in and will have NaNs for those lagged features\n",
    "- when shifting the time series forward by 168 hrs(7 days), the last 168 rows of each zone_id have \"no future data\" to shift into them.\n",
    "\n",
    "So:\n",
    "    - for station_1_lag168, station_2_lag168, ..., station_11_lag168, the first 168 timestamps for each zone_id will be NaN.\n",
    "    - for is_day_before_holiday_lead168, is_day_after_holiday_lead168, the last 168 timestamps for each zone_id will be NaN.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# Missing values percentage is 0.02% therefore we can drop the rows with NaN\n",
    "features_with_lags = features_with_lags.dropna().reset_index(drop=True)\n",
    "print((features_with_lags).isna().sum().sort_values(ascending=False).head(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "c9f23027",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "zone_id                          0\n",
       "date_time                        0\n",
       "load                             0\n",
       "station_1                        0\n",
       "station_2                        0\n",
       "                                ..\n",
       "CDD_roll168                      0\n",
       "is_day_before_holiday_lead24     0\n",
       "is_day_after_holiday_lead24      0\n",
       "is_day_before_holiday_lead168    0\n",
       "is_day_after_holiday_lead168     0\n",
       "Length: 164, dtype: int64"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check for infinite values\n",
    "np.isinf(features_with_lags).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "eb93dd1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   zone_id           date_time     load  station_1  station_2  station_3  \\\n",
      "0        1 2004-01-08 00:00:00  25104.0       29.0       22.0       21.0   \n",
      "1        1 2004-01-08 01:00:00  25122.0       26.0       22.0       20.0   \n",
      "2        1 2004-01-08 02:00:00  25464.0       24.0       21.0       20.0   \n",
      "3        1 2004-01-08 03:00:00  25715.0       22.0       21.0       19.0   \n",
      "4        1 2004-01-08 04:00:00  26219.0       22.0       22.0       18.0   \n",
      "\n",
      "   station_4  station_5  station_6  station_7  ...  station_9_roll168  \\\n",
      "0       25.0       23.0       25.0       21.0  ...          44.654762   \n",
      "1       26.0       24.0       26.0       21.0  ...          44.565476   \n",
      "2       25.0       23.0       25.0       20.0  ...          44.482143   \n",
      "3       22.0       24.0       26.0       20.0  ...          44.398810   \n",
      "4       20.0       24.0       27.0       21.0  ...          44.339286   \n",
      "\n",
      "   station_10_roll168  station_11_roll168  temp_mean_roll168  HDD_roll168  \\\n",
      "0           48.976190           45.327381          46.799784          0.0   \n",
      "1           48.875000           45.255952          46.694805          0.0   \n",
      "2           48.773810           45.184524          46.594697          0.0   \n",
      "3           48.696429           45.119048          46.511364          0.0   \n",
      "4           48.648810           45.029762          46.439394          0.0   \n",
      "\n",
      "   CDD_roll168  is_day_before_holiday_lead24  is_day_after_holiday_lead24  \\\n",
      "0    28.799784                           0.0                          0.0   \n",
      "1    28.694805                           0.0                          0.0   \n",
      "2    28.594697                           0.0                          0.0   \n",
      "3    28.511364                           0.0                          0.0   \n",
      "4    28.439394                           0.0                          0.0   \n",
      "\n",
      "   is_day_before_holiday_lead168  is_day_after_holiday_lead168  \n",
      "0                            0.0                           0.0  \n",
      "1                            0.0                           0.0  \n",
      "2                            0.0                           0.0  \n",
      "3                            0.0                           0.0  \n",
      "4                            0.0                           0.0  \n",
      "\n",
      "[5 rows x 164 columns]\n"
     ]
    }
   ],
   "source": [
    "print(features_with_lags.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18eeecf6",
   "metadata": {},
   "source": [
    "#### In terms of forecasting models, ARIMA is already a \"white box\" so there is no need to apply SHAP and LIME, however if we want to locally explain a prediction, that would need to be computed that manually.\n",
    "#### ARIMA does not need to have manually lagged features and assumes only one variable, therefore for our timeseries with multiple variables, we would need to use SARIMAX which is a seasonal ARIMA with exogenous variables.\n",
    "#### In terms of explainability techniques, in a SARIMAX model, the coeficients themselves are the explanations, where in a XGBoost (and other ML models), SHAP or LIME can explain which features influence each prediction, whether they increased or decreased the forecast and how strongly the features contributed to the forecast."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8489c11e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NaNs per exog column:\n",
      " temp_mean     0\n",
      "HDD           0\n",
      "CDD           0\n",
      "is_holiday    0\n",
      "dtype: int64\n",
      "Infs per exog column:\n",
      " temp_mean     0\n",
      "HDD           0\n",
      "CDD           0\n",
      "is_holiday    0\n",
      "dtype: int64\n",
      "                                     SARIMAX Results                                      \n",
      "==========================================================================================\n",
      "Dep. Variable:                               load   No. Observations:                39096\n",
      "Model:             SARIMAX(1, 1, 1)x(1, 0, 1, 24)   Log Likelihood             -303356.980\n",
      "Date:                            Thu, 09 Oct 2025   AIC                         606731.960\n",
      "Time:                                    09:22:13   BIC                         606809.117\n",
      "Sample:                                01-08-2004   HQIC                        606756.409\n",
      "                                     - 06-23-2008                                         \n",
      "Covariance Type:                              opg                                         \n",
      "==============================================================================\n",
      "                 coef    std err          z      P>|z|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "temp_mean   -380.2130    228.050     -1.667      0.095    -827.183      66.757\n",
      "HDD          731.0767    228.583      3.198      0.001     283.062    1179.091\n",
      "CDD          350.6773    228.138      1.537      0.124     -96.465     797.820\n",
      "is_holiday   -11.9077     44.108     -0.270      0.787     -98.359      74.543\n",
      "ar.L1          0.3061      0.004     68.085      0.000       0.297       0.315\n",
      "ma.L1          0.4227      0.004    108.668      0.000       0.415       0.430\n",
      "ar.S.L24       0.9936      0.001   1795.672      0.000       0.993       0.995\n",
      "ma.S.L24      -0.8481      0.002   -383.203      0.000      -0.852      -0.844\n",
      "sigma2      3.245e+05   1227.789    264.259      0.000    3.22e+05    3.27e+05\n",
      "===================================================================================\n",
      "Ljung-Box (L1) (Q):                   1.38   Jarque-Bera (JB):            119482.63\n",
      "Prob(Q):                              0.24   Prob(JB):                         0.00\n",
      "Heteroskedasticity (H):               1.17   Skew:                             0.10\n",
      "Prob(H) (two-sided):                  0.00   Kurtosis:                        11.56\n",
      "===================================================================================\n",
      "\n",
      "Warnings:\n",
      "[1] Covariance matrix calculated using the outer product of gradients (complex-step).\n"
     ]
    }
   ],
   "source": [
    "# Goal is forecast accuracy + feature interpretability, therefore the strategy is to start with SARIMAX model to get baseline interpretability and understand structure. \n",
    "# Move to XGBoost for better accuracy and use SHAP for interpretability\n",
    "\n",
    "# Quick test on a single zone\n",
    "# 1) Filter one zone and enforce hourly freq\n",
    "zone = 1\n",
    "df_zone = features_with_lags.loc[features_with_lags[\"zone_id\"] == zone].copy()\n",
    "df_zone = df_zone.sort_values(\"date_time\").set_index(\"date_time\").asfreq(\"h\")\n",
    "\n",
    "# 2) Select target and exogenous columns\n",
    "cols_exog = [\"temp_mean\", \"HDD\", \"CDD\", \"is_holiday\"]  # adjust if your column names differ\n",
    "missing_cols = [c for c in cols_exog if c not in df_zone.columns]\n",
    "if missing_cols:\n",
    "    raise KeyError(f\"Exogenous columns missing: {missing_cols}\")\n",
    "\n",
    "y = df_zone[\"load\"]\n",
    "X = df_zone[cols_exog].copy()\n",
    "\n",
    "# 3) Make sure exog are numeric (objects/strings -> NaN)\n",
    "X = X.apply(pd.to_numeric, errors=\"coerce\")\n",
    "# (optional) if is_holiday is boolean/object, make it 0/1\n",
    "if \"is_holiday\" in X:\n",
    "    X[\"is_holiday\"] = X[\"is_holiday\"].fillna(0).astype(int)\n",
    "\n",
    "# 4) Quick diagnostics (see which columns have issues)\n",
    "print(\"NaNs per exog column:\\n\", X.isna().sum())\n",
    "print(\"Infs per exog column:\\n\", np.isinf(X).sum())\n",
    "\n",
    "# 5) Build a clean mask (drop any row with NaN/inf in y or exog)\n",
    "mask = (\n",
    "    y.notna() &\n",
    "    np.isfinite(y) &\n",
    "    X.notna().all(axis=1) &\n",
    "    np.isfinite(X).all(axis=1)\n",
    ")\n",
    "\n",
    "y_clean = y[mask]\n",
    "X_clean = X[mask]\n",
    "\n",
    "# 6) Fit SARIMAX on the clean, aligned data\n",
    "model = sm.tsa.SARIMAX(\n",
    "    endog=y_clean,\n",
    "    exog=X_clean,\n",
    "    order=(1,1,1),\n",
    "    seasonal_order=(1,0,1,24),\n",
    "    enforce_stationarity=False,\n",
    "    enforce_invertibility=False,\n",
    ")\n",
    "res = model.fit(disp=False)\n",
    "print(res.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45a8de53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using the full dataset, but split chronologically into train/valid/test sets\n",
    "\n",
    "# Set the chronological cutoffs (inclusive)\n",
    "TRAIN_END = \"2006-12-31 23:00\"\n",
    "VALID_END = \"2007-12-31 23:00\"\n",
    "DATE_COL  = \"date_time\"\n",
    "ZONE_COL  = \"zone_id\"\n",
    "TARGET    = \"load\"\n",
    "SEASONAL_PERIOD = 24   # hourly daily seasonality\n",
    "\n",
    "# ====== METRICS ======\n",
    "def rmse(y_true: pd.Series, y_pred: pd.Series) -> float:\n",
    "    y_true, y_pred = y_true.align(y_pred, join=\"inner\")\n",
    "    return float(np.sqrt(mean_squared_error(y_true, y_pred)))\n",
    "\n",
    "def mae(y_true: pd.Series, y_pred: pd.Series) -> float:\n",
    "    y_true, y_pred = y_true.align(y_pred, join=\"inner\")\n",
    "    return float(mean_absolute_error(y_true, y_pred))\n",
    "\n",
    "def smape(y_true: pd.Series, y_pred: pd.Series) -> float:\n",
    "    y_true, y_pred = y_true.align(y_pred, join=\"inner\")\n",
    "    denom = np.clip(np.abs(y_true.values) + np.abs(y_pred.values), 1e-9, None)\n",
    "    return float(np.mean(2.0 * np.abs(y_pred.values - y_true.values) / denom) * 100.0)\n",
    "\n",
    "# ====== EXOG COLUMN PICKER ======\n",
    "def pick_exog_columns(df: pd.DataFrame) -> List[str]:\n",
    "    \"\"\"Auto-pick exogenous drivers from the schema.\"\"\"\n",
    "    keep_exact = {\n",
    "        \"temp_mean\", \"HDD\", \"CDD\",\n",
    "        \"is_holiday\", \"is_day_before_holiday\", \"is_day_after_holiday\",\n",
    "        \"sunrise_hour_approx\", \"sunset_hour_approx\",\n",
    "        \"daylight_hours_approx\", \"is_daylight_approx\", \"daylight_proxy\",\n",
    "    }\n",
    "    cols: List[str] = []\n",
    "    for c in df.columns:\n",
    "        if c in (ZONE_COL, DATE_COL, TARGET):\n",
    "            continue\n",
    "        # station temps + any engineered lag/roll/lead and the exact extras\n",
    "        if c.startswith(\"station_\") or c in keep_exact or any(tok in c for tok in (\"_lag\", \"_roll\", \"_lead\")):\n",
    "            cols.append(c)\n",
    "    # de-dup preserve order\n",
    "    return list(dict.fromkeys(cols))\n",
    "\n",
    "# ====== DATA PREP PER ZONE ======\n",
    "def prepare_splits_one_zone(df_zone: pd.DataFrame, exog_cols: List[str]\n",
    ") -> Tuple[Tuple[pd.Series, pd.DataFrame], Tuple[pd.Series, pd.DataFrame], Tuple[pd.Series, pd.DataFrame]]:\n",
    "    \"\"\"\n",
    "    Returns: (y_train, X_train), (y_valid, X_valid), (y_test, X_test)\n",
    "    with an hourly DatetimeIndex and NA rows removed.\n",
    "    \"\"\"\n",
    "    dfz = (df_zone\n",
    "           .sort_values(DATE_COL)\n",
    "           .set_index(DATE_COL)\n",
    "           .asfreq(\"h\"))  # enforce hourly frequency; gaps become NaN\n",
    "\n",
    "    y = dfz[TARGET]\n",
    "    X = dfz[exog_cols].copy()\n",
    "\n",
    "    # Coerce exog to numeric and normalize holiday dtypes if present\n",
    "    X = X.apply(pd.to_numeric, errors=\"coerce\")\n",
    "    if \"is_holiday\" in X.columns:\n",
    "        X[\"is_holiday\"] = X[\"is_holiday\"].fillna(0).astype(int)\n",
    "\n",
    "    # Drop any row with NA in y or exog (common after asfreq/lag windows)\n",
    "    mask = y.notna() & X.notna().all(axis=1)\n",
    "    y, X = y[mask], X[mask]\n",
    "\n",
    "    # Chronological splits\n",
    "    y_tr, X_tr = y.loc[:TRAIN_END], X.loc[:TRAIN_END]\n",
    "    y_va, X_va = y.loc[TRAIN_END:VALID_END].iloc[1:], X.loc[TRAIN_END:VALID_END].iloc[1:]  # drop overlap row\n",
    "    y_te, X_te = y.loc[VALID_END:].iloc[1:],           X.loc[VALID_END:].iloc[1:]\n",
    "\n",
    "    return (y_tr, X_tr), (y_va, X_va), (y_te, X_te)\n",
    "\n",
    "# ====== FIT + FORECAST HELPERS ======\n",
    "def fit_sarimax(y_train: pd.Series,\n",
    "                X_train: pd.DataFrame,\n",
    "                order=(1,1,1),\n",
    "                seasonal_order=(1,0,1,SEASONAL_PERIOD)):\n",
    "    model = sm.tsa.SARIMAX(\n",
    "        endog=y_train,\n",
    "        exog=None if X_train.empty else X_train,\n",
    "        order=order,\n",
    "        seasonal_order=seasonal_order,\n",
    "        enforce_stationarity=False,\n",
    "        enforce_invertibility=False,\n",
    "    )\n",
    "    return model.fit(disp=False)\n",
    "\n",
    "def forecast_with_exog(res, X_future: pd.DataFrame) -> pd.Series:\n",
    "    steps = len(X_future)\n",
    "    if steps == 0:\n",
    "        return pd.Series(dtype=float)\n",
    "    fc = res.get_forecast(steps=steps, exog=None if X_future.empty else X_future).predicted_mean\n",
    "    fc.index = X_future.index\n",
    "    return fc\n",
    "\n",
    "# ====== MAIN: SPLIT, FIT, EVAL PER ZONE ======\n",
    "def run_sarimax_per_zone(features_with_lags: pd.DataFrame,\n",
    "                         order=(1,1,1),\n",
    "                         seasonal_order=(1,0,1,SEASONAL_PERIOD)\n",
    ") -> Tuple[Dict[int, sm.tsa.statespace.sarimax.SARIMAXResults], pd.DataFrame]:\n",
    "    exog_cols = pick_exog_columns(features_with_lags)\n",
    "    zones = np.sort(features_with_lags[ZONE_COL].dropna().unique())\n",
    "\n",
    "    models: Dict[int, sm.tsa.statespace.sarimax.SARIMAXResults] = {}\n",
    "    rows = []\n",
    "\n",
    "    for z in zones:\n",
    "        df_zone = features_with_lags[features_with_lags[ZONE_COL] == z].copy()\n",
    "        (y_tr, X_tr), (y_va, X_va), (y_te, X_te) = prepare_splits_one_zone(df_zone, exog_cols)\n",
    "\n",
    "        # If not enough data remains after cleaning, skip\n",
    "        if len(y_tr) == 0 or len(y_va) == 0:\n",
    "            print(f\"[Zone {z}] Skipped: insufficient clean data after split.\")\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            res = fit_sarimax(y_tr, X_tr, order=order, seasonal_order=seasonal_order)\n",
    "        except Exception as e:\n",
    "            print(f\"[Zone {z}] SARIMAX fit failed: {e}\")\n",
    "            continue\n",
    "\n",
    "        # Forecast and evaluate on validation\n",
    "        fc_va = forecast_with_exog(res, X_va)\n",
    "        val_mae  = mae(y_va, fc_va)\n",
    "        val_rmse = rmse(y_va, fc_va)\n",
    "        val_smape = smape(y_va, fc_va)\n",
    "\n",
    "        # Optional: evaluate on test as well\n",
    "        fc_te = forecast_with_exog(res, X_te)\n",
    "        test_mae  = mae(y_te, fc_te)  if len(y_te) else np.nan\n",
    "        test_rmse = rmse(y_te, fc_te) if len(y_te) else np.nan\n",
    "        test_smape = smape(y_te, fc_te) if len(y_te) else np.nan\n",
    "\n",
    "        rows.append({\n",
    "            \"zone_id\": int(z),\n",
    "            \"AIC\": float(res.aic),\n",
    "            \"VAL_MAE\":  val_mae,   \"VAL_RMSE\":  val_rmse,   \"VAL_sMAPE_%\":  val_smape,\n",
    "            \"TEST_MAE\": test_mae,  \"TEST_RMSE\": test_rmse,  \"TEST_sMAPE_%\": test_smape,\n",
    "        })\n",
    "        models[int(z)] = res\n",
    "\n",
    "        print(f\"[Zone {z}]  VAL  RMSE={val_rmse:.2f}  MAE={val_mae:.2f}  sMAPE={val_smape:.2f}%  \"\n",
    "              f\"| TEST RMSE={test_rmse:.2f}  MAE={test_mae:.2f}  sMAPE={test_smape:.2f}%\")\n",
    "\n",
    "    results = pd.DataFrame(rows).sort_values(\"zone_id\").reset_index(drop=True)\n",
    "    return models, results\n",
    "\n",
    "# ====== RUN ======\n",
    "# Ensure the DataFrame is sorted before splitting\n",
    "features_with_lags = features_with_lags.sort_values([ZONE_COL, DATE_COL]).reset_index(drop=True)\n",
    "\n",
    "models_by_zone, metrics_table = run_sarimax_per_zone(\n",
    "    features_with_lags,\n",
    "    order=(1,1,1),            # start simple; tune later\n",
    "    seasonal_order=(1,0,1,24) # daily (24h) seasonality\n",
    ")\n",
    "\n",
    "print(\"\\n=== Metrics (head) ===\")\n",
    "print(metrics_table.head(10))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "explainable_ai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

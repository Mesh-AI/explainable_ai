{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a17c0fea",
   "metadata": {},
   "source": [
    "## Load and clean the datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "46bcd565",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import neccessary libarries\n",
    "\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "import re\n",
    "from math import pi\n",
    "from typing import Iterable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a03b6f5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load the data\n",
    "base_dir = Path('/Users/madalinaciolan/dev/explainable_ai')\n",
    "\n",
    "#load the energy demand data(load)\n",
    "load_path = base_dir / 'data' / 'Load' / 'load_history.csv'\n",
    "load_df = pd.read_csv(load_path)\n",
    "\n",
    "#load the temperature data\n",
    "temp_path = base_dir / 'data' / 'Load' / 'temperature_history.csv'\n",
    "temp_df = pd.read_csv(temp_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "acfe5dcc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   zone_id  year  month  day      h1      h2      h3      h4      h5      h6  \\\n",
      "1        1  2004      1    1  16,853  16,450  16,517  16,873  17,064  17,727   \n",
      "2        1  2004      1    2  14,155  14,038  14,019  14,489  14,920  16,072   \n",
      "3        1  2004      1    3  14,439  14,272  14,109  14,081  14,775  15,491   \n",
      "4        1  2004      1    4  11,273  10,415   9,943   9,859   9,881  10,248   \n",
      "5        1  2004      1    5  10,750  10,321  10,107  10,065  10,419  12,101   \n",
      "\n",
      "   ...     h15     h16     h17     h18     h19     h20     h21     h22  \\\n",
      "1  ...  13,518  13,138  14,130  16,809  18,150  18,235  17,925  16,904   \n",
      "2  ...  16,127  15,448  15,839  17,727  18,895  18,650  18,443  17,580   \n",
      "3  ...  13,507  13,414  13,826  15,825  16,996  16,394  15,406  14,278   \n",
      "4  ...  14,207  13,614  14,162  16,237  17,430  17,218  16,633  15,238   \n",
      "5  ...  13,845  14,350  15,501  17,307  18,786  19,089  19,192  18,416   \n",
      "\n",
      "      h23     h24  \n",
      "1  16,162  14,750  \n",
      "2  16,467  15,258  \n",
      "3  13,315  12,424  \n",
      "4  13,580  11,727  \n",
      "5  17,006  16,018  \n",
      "\n",
      "[5 rows x 28 columns]\n",
      "   station_id  year  month  day  h1  h2  h3  h4  h5  h6  ...   h15   h16  \\\n",
      "1           1  2004      1    1  46  46  45  41  39  35  ...  55.0  55.0   \n",
      "2           1  2004      1    2  43  44  46  46  47  47  ...  54.0  56.0   \n",
      "3           1  2004      1    3  45  46  46  44  43  46  ...  69.0  68.0   \n",
      "4           1  2004      1    4  63  62  62  62  60  60  ...  71.0  72.0   \n",
      "5           1  2004      1    5  64  63  65  64  64  64  ...  66.0  66.0   \n",
      "\n",
      "    h17   h18   h19   h20   h21   h22   h23   h24  \n",
      "1  52.0  46.0  40.0  40.0  39.0  38.0  40.0  41.0  \n",
      "2  57.0  53.0  50.0  47.0  46.0  45.0  45.0  45.0  \n",
      "3  68.0  65.0  64.0  63.0  62.0  63.0  63.0  62.0  \n",
      "4  71.0  68.0  67.0  67.0  65.0  64.0  65.0  64.0  \n",
      "5  66.0  66.0  63.0  54.0  52.0  49.0  47.0  47.0  \n",
      "\n",
      "[5 rows x 28 columns]\n"
     ]
    }
   ],
   "source": [
    "#fix the index for both dataframes\n",
    "load_df.index = range(1, len(load_df) + 1)\n",
    "print(load_df.head())\n",
    "\n",
    "temp_df.index = range(1, len(temp_df) + 1)\n",
    "print(temp_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "5fc20b43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 33000 entries, 1 to 33000\n",
      "Data columns (total 28 columns):\n",
      " #   Column   Non-Null Count  Dtype \n",
      "---  ------   --------------  ----- \n",
      " 0   zone_id  33000 non-null  int64 \n",
      " 1   year     33000 non-null  int64 \n",
      " 2   month    33000 non-null  int64 \n",
      " 3   day      33000 non-null  int64 \n",
      " 4   h1       31740 non-null  object\n",
      " 5   h2       31740 non-null  object\n",
      " 6   h3       31740 non-null  object\n",
      " 7   h4       31740 non-null  object\n",
      " 8   h5       31740 non-null  object\n",
      " 9   h6       31740 non-null  object\n",
      " 10  h7       31720 non-null  object\n",
      " 11  h8       31720 non-null  object\n",
      " 12  h9       31720 non-null  object\n",
      " 13  h10      31720 non-null  object\n",
      " 14  h11      31720 non-null  object\n",
      " 15  h12      31720 non-null  object\n",
      " 16  h13      31720 non-null  object\n",
      " 17  h14      31720 non-null  object\n",
      " 18  h15      31720 non-null  object\n",
      " 19  h16      31720 non-null  object\n",
      " 20  h17      31720 non-null  object\n",
      " 21  h18      31720 non-null  object\n",
      " 22  h19      31720 non-null  object\n",
      " 23  h20      31720 non-null  object\n",
      " 24  h21      31720 non-null  object\n",
      " 25  h22      31720 non-null  object\n",
      " 26  h23      31720 non-null  object\n",
      " 27  h24      31720 non-null  object\n",
      "dtypes: int64(4), object(24)\n",
      "memory usage: 7.0+ MB\n",
      "None\n",
      "zone_id       0\n",
      "year          0\n",
      "month         0\n",
      "day           0\n",
      "h1         1260\n",
      "h2         1260\n",
      "h3         1260\n",
      "h4         1260\n",
      "h5         1260\n",
      "h6         1260\n",
      "h7         1280\n",
      "h8         1280\n",
      "h9         1280\n",
      "h10        1280\n",
      "h11        1280\n",
      "h12        1280\n",
      "h13        1280\n",
      "h14        1280\n",
      "h15        1280\n",
      "h16        1280\n",
      "h17        1280\n",
      "h18        1280\n",
      "h19        1280\n",
      "h20        1280\n",
      "h21        1280\n",
      "h22        1280\n",
      "h23        1280\n",
      "h24        1280\n",
      "dtype: int64\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 18073 entries, 1 to 18073\n",
      "Data columns (total 28 columns):\n",
      " #   Column      Non-Null Count  Dtype  \n",
      "---  ------      --------------  -----  \n",
      " 0   station_id  18073 non-null  int64  \n",
      " 1   year        18073 non-null  int64  \n",
      " 2   month       18073 non-null  int64  \n",
      " 3   day         18073 non-null  int64  \n",
      " 4   h1          18073 non-null  int64  \n",
      " 5   h2          18073 non-null  int64  \n",
      " 6   h3          18073 non-null  int64  \n",
      " 7   h4          18073 non-null  int64  \n",
      " 8   h5          18073 non-null  int64  \n",
      " 9   h6          18073 non-null  int64  \n",
      " 10  h7          18062 non-null  float64\n",
      " 11  h8          18062 non-null  float64\n",
      " 12  h9          18062 non-null  float64\n",
      " 13  h10         18062 non-null  float64\n",
      " 14  h11         18062 non-null  float64\n",
      " 15  h12         18062 non-null  float64\n",
      " 16  h13         18062 non-null  float64\n",
      " 17  h14         18062 non-null  float64\n",
      " 18  h15         18062 non-null  float64\n",
      " 19  h16         18062 non-null  float64\n",
      " 20  h17         18062 non-null  float64\n",
      " 21  h18         18062 non-null  float64\n",
      " 22  h19         18062 non-null  float64\n",
      " 23  h20         18062 non-null  float64\n",
      " 24  h21         18062 non-null  float64\n",
      " 25  h22         18062 non-null  float64\n",
      " 26  h23         18062 non-null  float64\n",
      " 27  h24         18062 non-null  float64\n",
      "dtypes: float64(18), int64(10)\n",
      "memory usage: 3.9 MB\n",
      "None\n",
      "station_id     0\n",
      "year           0\n",
      "month          0\n",
      "day            0\n",
      "h1             0\n",
      "h2             0\n",
      "h3             0\n",
      "h4             0\n",
      "h5             0\n",
      "h6             0\n",
      "h7            11\n",
      "h8            11\n",
      "h9            11\n",
      "h10           11\n",
      "h11           11\n",
      "h12           11\n",
      "h13           11\n",
      "h14           11\n",
      "h15           11\n",
      "h16           11\n",
      "h17           11\n",
      "h18           11\n",
      "h19           11\n",
      "h20           11\n",
      "h21           11\n",
      "h22           11\n",
      "h23           11\n",
      "h24           11\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "#check the info and missing values for both dataframes\n",
    "print(load_df.info())\n",
    "print(load_df.isna().sum())\n",
    "\n",
    "print(temp_df.info())\n",
    "print(temp_df.isna().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "4fe29248",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zone_id      int64\n",
      "year         int64\n",
      "month        int64\n",
      "day          int64\n",
      "h1         float64\n",
      "h2         float64\n",
      "h3         float64\n",
      "h4         float64\n",
      "h5         float64\n",
      "h6         float64\n",
      "h7         float64\n",
      "h8         float64\n",
      "h9         float64\n",
      "h10        float64\n",
      "h11        float64\n",
      "h12        float64\n",
      "h13        float64\n",
      "h14        float64\n",
      "h15        float64\n",
      "h16        float64\n",
      "h17        float64\n",
      "h18        float64\n",
      "h19        float64\n",
      "h20        float64\n",
      "h21        float64\n",
      "h22        float64\n",
      "h23        float64\n",
      "h24        float64\n",
      "dtype: object\n",
      "zone_id       0\n",
      "year          0\n",
      "month         0\n",
      "day           0\n",
      "h1         1260\n",
      "h2         1260\n",
      "h3         1260\n",
      "h4         1260\n",
      "h5         1260\n",
      "h6         1260\n",
      "h7         1280\n",
      "h8         1280\n",
      "h9         1280\n",
      "h10        1280\n",
      "h11        1280\n",
      "h12        1280\n",
      "h13        1280\n",
      "h14        1280\n",
      "h15        1280\n",
      "h16        1280\n",
      "h17        1280\n",
      "h18        1280\n",
      "h19        1280\n",
      "h20        1280\n",
      "h21        1280\n",
      "h22        1280\n",
      "h23        1280\n",
      "h24        1280\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# ---- Clean the load dataframe ----   \n",
    "load_df_clean = load_df.copy()\n",
    "cols = [f\"h{i}\" for i in range(1, 25)]\n",
    "for col in cols:\n",
    "    load_df_clean[col] = load_df_clean[col].str.replace(\",\", \"\").astype(float)\n",
    "\n",
    "print(load_df_clean.dtypes)\n",
    "print(load_df_clean.isna().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "791a3b79",
   "metadata": {},
   "source": [
    "## Fill in missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "86fed1df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Remaining NaNs per column after interpolation:  h1     1260\n",
      "h2     1260\n",
      "h3     1260\n",
      "h4     1260\n",
      "h5     1260\n",
      "h6     1260\n",
      "h7     1260\n",
      "h8     1260\n",
      "h9     1260\n",
      "h10    1260\n",
      "h11    1260\n",
      "h12    1260\n",
      "h13    1260\n",
      "h14    1260\n",
      "h15    1260\n",
      "h16    1260\n",
      "h17    1260\n",
      "h18    1260\n",
      "h19    1260\n",
      "h20    1260\n",
      "h21    1260\n",
      "h22    1260\n",
      "h23    1260\n",
      "h24    1260\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# --- Fill NaNs in load_df_clean using interpolation before/after ---\n",
    "# 1) Horizontal interpolation within each day across h1..h24\n",
    "#    (fills gaps inside a day's 24 hours using surrounding hours)\n",
    "load_df_clean[cols] = load_df_clean[cols].interpolate(method='linear', axis=1, limit_direction='both')\n",
    "\n",
    "# 2) Vertical interpolation across days for each hour h1..h24\n",
    "#    (fills gaps across days using surrounding days for each hour)\n",
    "#load_df_clean[cols] = load_df_clean[cols].interpolate(method='linear',  axis=0, limit_direction='both')\n",
    "\n",
    "# check if all NaNs in the load data frame have been filled\n",
    "print(\"\\nRemaining NaNs per column after interpolation: \", load_df_clean[cols].isna().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "eac8d82f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zone_id      int64\n",
      "year         int64\n",
      "month        int64\n",
      "day          int64\n",
      "h1         float64\n",
      "h2         float64\n",
      "h3         float64\n",
      "h4         float64\n",
      "h5         float64\n",
      "h6         float64\n",
      "h7         float64\n",
      "h8         float64\n",
      "h9         float64\n",
      "h10        float64\n",
      "h11        float64\n",
      "h12        float64\n",
      "h13        float64\n",
      "h14        float64\n",
      "h15        float64\n",
      "h16        float64\n",
      "h17        float64\n",
      "h18        float64\n",
      "h19        float64\n",
      "h20        float64\n",
      "h21        float64\n",
      "h22        float64\n",
      "h23        float64\n",
      "h24        float64\n",
      "dtype: object\n",
      "station_id     0\n",
      "year           0\n",
      "month          0\n",
      "day            0\n",
      "h1             0\n",
      "h2             0\n",
      "h3             0\n",
      "h4             0\n",
      "h5             0\n",
      "h6             0\n",
      "h7            11\n",
      "h8            11\n",
      "h9            11\n",
      "h10           11\n",
      "h11           11\n",
      "h12           11\n",
      "h13           11\n",
      "h14           11\n",
      "h15           11\n",
      "h16           11\n",
      "h17           11\n",
      "h18           11\n",
      "h19           11\n",
      "h20           11\n",
      "h21           11\n",
      "h22           11\n",
      "h23           11\n",
      "h24           11\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# ---- Clean the temperature dataframe ----  \n",
    "print(load_df_clean.dtypes) #no need to change types, they are already integers and floats\n",
    "print(temp_df.isna().sum()) #NaNs only in temperature colums h7 -> h24\n",
    "temp_df_clean = temp_df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "af9ff0e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Fill NaNs in load_df_clean using interpolation before/after ---\n",
    "#  Vertical interpolation across days for each hour h7..h24\n",
    "#    (fills gaps across days using surrounding days for each hour)\n",
    "cols_temp = [f\"h{i}\" for i in range(7, 25)]\n",
    "for col in cols_temp:\n",
    "    temp_df_clean[col] = temp_df_clean[col].interpolate(method='linear', axis=0, limit_direction='both')    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "2114585e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Remaining NaNs per column after interpolation:  station_id    0\n",
      "year          0\n",
      "month         0\n",
      "day           0\n",
      "h1            0\n",
      "h2            0\n",
      "h3            0\n",
      "h4            0\n",
      "h5            0\n",
      "h6            0\n",
      "h7            0\n",
      "h8            0\n",
      "h9            0\n",
      "h10           0\n",
      "h11           0\n",
      "h12           0\n",
      "h13           0\n",
      "h14           0\n",
      "h15           0\n",
      "h16           0\n",
      "h17           0\n",
      "h18           0\n",
      "h19           0\n",
      "h20           0\n",
      "h21           0\n",
      "h22           0\n",
      "h23           0\n",
      "h24           0\n",
      "dtype: int64\n",
      "   zone_id  year  month  day       h1       h2       h3       h4       h5  \\\n",
      "1        1  2004      1    1  16853.0  16450.0  16517.0  16873.0  17064.0   \n",
      "2        1  2004      1    2  14155.0  14038.0  14019.0  14489.0  14920.0   \n",
      "3        1  2004      1    3  14439.0  14272.0  14109.0  14081.0  14775.0   \n",
      "4        1  2004      1    4  11273.0  10415.0   9943.0   9859.0   9881.0   \n",
      "5        1  2004      1    5  10750.0  10321.0  10107.0  10065.0  10419.0   \n",
      "\n",
      "        h6  ...      h15      h16      h17      h18      h19      h20  \\\n",
      "1  17727.0  ...  13518.0  13138.0  14130.0  16809.0  18150.0  18235.0   \n",
      "2  16072.0  ...  16127.0  15448.0  15839.0  17727.0  18895.0  18650.0   \n",
      "3  15491.0  ...  13507.0  13414.0  13826.0  15825.0  16996.0  16394.0   \n",
      "4  10248.0  ...  14207.0  13614.0  14162.0  16237.0  17430.0  17218.0   \n",
      "5  12101.0  ...  13845.0  14350.0  15501.0  17307.0  18786.0  19089.0   \n",
      "\n",
      "       h21      h22      h23      h24  \n",
      "1  17925.0  16904.0  16162.0  14750.0  \n",
      "2  18443.0  17580.0  16467.0  15258.0  \n",
      "3  15406.0  14278.0  13315.0  12424.0  \n",
      "4  16633.0  15238.0  13580.0  11727.0  \n",
      "5  19192.0  18416.0  17006.0  16018.0  \n",
      "\n",
      "[5 rows x 28 columns]\n"
     ]
    }
   ],
   "source": [
    "#check if all NaNs in the temperature data frame have been filled\n",
    "print(\"\\nRemaining NaNs per column after interpolation: \", temp_df_clean.isna().sum()) \n",
    "\n",
    "print(load_df_clean.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03ba4a64",
   "metadata": {},
   "source": [
    "## Reshape both data frames from wide to long format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "9c3c7b41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   zone_id           date_time     load\n",
      "0        1 2004-01-01 00:00:00  16853.0\n",
      "1        1 2004-01-01 01:00:00  16450.0\n",
      "2        1 2004-01-01 02:00:00  16517.0\n",
      "3        1 2004-01-01 03:00:00  16873.0\n",
      "4        1 2004-01-01 04:00:00  17064.0\n"
     ]
    }
   ],
   "source": [
    "# ----Reshaping the load dataframe from wide to long format ----\n",
    "\n",
    "# Melt the dataframe to have a long format with columns: year, month, day, hour, load\n",
    "hours_cols = [f\"h{i}\" for i in range(1, 25)]\n",
    "load_long = load_df_clean.melt(\n",
    "    id_vars=[\"zone_id\", \"year\", \"month\", \"day\"],\n",
    "    value_vars=hours_cols,\n",
    "    var_name=\"hour\",\n",
    "    value_name=\"load\",\n",
    ")\n",
    "\n",
    "# Convert hour column to numeric hour\n",
    "load_long['hour'] = load_long['hour'].str.extract('h(\\d+)').astype(int)\n",
    "\n",
    "# Create a datetime column\n",
    "load_long['date_time'] = pd.to_datetime(load_long[['year', 'month', 'day']]) + pd.to_timedelta(load_long['hour'] - 1, unit='h')\n",
    "\n",
    "#Keep only relevant columns and sort by date\n",
    "load_long = load_long[['zone_id','date_time', 'load']].sort_values(['zone_id', 'date_time']).reset_index(drop=True)\n",
    "print(load_long.head())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "9121858d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   station_id  year  month  day  h1  h2  h3  h4  h5  h6  ...   h15   h16  \\\n",
      "1           1  2004      1    1  46  46  45  41  39  35  ...  55.0  55.0   \n",
      "2           1  2004      1    2  43  44  46  46  47  47  ...  54.0  56.0   \n",
      "3           1  2004      1    3  45  46  46  44  43  46  ...  69.0  68.0   \n",
      "4           1  2004      1    4  63  62  62  62  60  60  ...  71.0  72.0   \n",
      "5           1  2004      1    5  64  63  65  64  64  64  ...  66.0  66.0   \n",
      "\n",
      "    h17   h18   h19   h20   h21   h22   h23   h24  \n",
      "1  52.0  46.0  40.0  40.0  39.0  38.0  40.0  41.0  \n",
      "2  57.0  53.0  50.0  47.0  46.0  45.0  45.0  45.0  \n",
      "3  68.0  65.0  64.0  63.0  62.0  63.0  63.0  62.0  \n",
      "4  71.0  68.0  67.0  67.0  65.0  64.0  65.0  64.0  \n",
      "5  66.0  66.0  63.0  54.0  52.0  49.0  47.0  47.0  \n",
      "\n",
      "[5 rows x 28 columns]\n",
      "   station_id  year  month  day  hour  temperature  date_time\n",
      "0           1  2004      1    1     1         46.0 2004-01-01\n",
      "1           1  2004      1    2     1         43.0 2004-01-02\n",
      "2           1  2004      1    3     1         45.0 2004-01-03\n",
      "3           1  2004      1    4     1         63.0 2004-01-04\n",
      "4           1  2004      1    5     1         64.0 2004-01-05\n"
     ]
    }
   ],
   "source": [
    "print(temp_df_clean.head())\n",
    "\n",
    "# ----Reshaping the temperature dataframe from wide to long format ----\n",
    "\n",
    "# Melt the dataframe to have a long format with columns: year, month, day, hour, temperature\n",
    "temp_hour_cols = [f\"h{i}\" for i in range(1, 25)]\n",
    "temp_long = temp_df_clean.melt(\n",
    "    id_vars=[\"station_id\", \"year\", \"month\", \"day\"],\n",
    "    value_vars=temp_hour_cols,\n",
    "    var_name=\"hour\",\n",
    "    value_name=\"temperature\",\n",
    ")\n",
    "\n",
    "# Convert hour column to numeric hour\n",
    "temp_long['hour'] = temp_long['hour'].str.extract('h(\\d+)').astype(int)\n",
    "\n",
    "# Create a datetime column\n",
    "temp_long['date_time'] = pd.to_datetime(temp_long[['year', 'month', 'day']]) + pd.to_timedelta(temp_long['hour'] - 1, unit='h')\n",
    "\n",
    "#Keep only relevant columns and sort by date\n",
    "#temp_long = temp_long[['station_id','date_time', 'temperature']].sort_values(by='date_time').reset_index(drop=True)\n",
    "#print(temp_long.head())\n",
    "\n",
    "# Pivot so each station is one column\n",
    "temp_wide = temp_long.pivot(index='date_time', columns='station_id', values='temperature')\n",
    "temp_wide.columns = [f\"station_{col}\" for col in temp_wide.columns]\n",
    "\n",
    "print(temp_long.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e062b92c",
   "metadata": {},
   "source": [
    "## Merge the dataframes \n",
    "### Two options: attach all of the stations' temperature to each load record or infer a map of correlations.\n",
    "### Have chosen the first option as it is recommended for modeling. I have merged the load data with all stations' temperatures so each load observation has temp_station_1, ... , temp_station_11 as features. This avoids the need to have a  zone <-> station map."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "c643b94b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        zone_id           date_time     load  station_1  station_2  station_3  \\\n",
      "0             1 2004-01-01 00:00:00  16853.0       46.0       38.0       44.0   \n",
      "1             1 2004-01-01 01:00:00  16450.0       46.0       36.0       42.0   \n",
      "2             1 2004-01-01 02:00:00  16517.0       45.0       35.0       40.0   \n",
      "3             1 2004-01-01 03:00:00  16873.0       41.0       30.0       36.0   \n",
      "4             1 2004-01-01 04:00:00  17064.0       39.0       30.0       34.0   \n",
      "...         ...                 ...      ...        ...        ...        ...   \n",
      "791995       20 2008-07-07 19:00:00      NaN        NaN        NaN        NaN   \n",
      "791996       20 2008-07-07 20:00:00      NaN        NaN        NaN        NaN   \n",
      "791997       20 2008-07-07 21:00:00      NaN        NaN        NaN        NaN   \n",
      "791998       20 2008-07-07 22:00:00      NaN        NaN        NaN        NaN   \n",
      "791999       20 2008-07-07 23:00:00      NaN        NaN        NaN        NaN   \n",
      "\n",
      "        station_4  station_5  station_6  station_7  station_8  station_9  \\\n",
      "0            45.0       42.0       44.0       45.0       43.0       41.0   \n",
      "1            43.0       42.0       43.0       44.0       44.0       39.0   \n",
      "2            41.0       40.0       42.0       41.0       42.0       36.0   \n",
      "3            37.0       39.0       38.0       40.0       34.0       35.0   \n",
      "4            33.0       40.0       38.0       35.0       30.0       33.0   \n",
      "...           ...        ...        ...        ...        ...        ...   \n",
      "791995        NaN        NaN        NaN        NaN        NaN        NaN   \n",
      "791996        NaN        NaN        NaN        NaN        NaN        NaN   \n",
      "791997        NaN        NaN        NaN        NaN        NaN        NaN   \n",
      "791998        NaN        NaN        NaN        NaN        NaN        NaN   \n",
      "791999        NaN        NaN        NaN        NaN        NaN        NaN   \n",
      "\n",
      "        station_10  station_11  \n",
      "0             42.0        36.0  \n",
      "1             43.0        32.0  \n",
      "2             43.0        31.0  \n",
      "3             39.0        30.0  \n",
      "4             35.0        34.0  \n",
      "...            ...         ...  \n",
      "791995         NaN         NaN  \n",
      "791996         NaN         NaN  \n",
      "791997         NaN         NaN  \n",
      "791998         NaN         NaN  \n",
      "791999         NaN         NaN  \n",
      "\n",
      "[792000 rows x 14 columns]\n"
     ]
    }
   ],
   "source": [
    "# --- Merge: attach all stations' temperatures to each load record ---\n",
    "merged_all = load_long.merge(temp_wide.reset_index(), on='date_time', how='left')\n",
    "print(merged_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "1df76c24",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "zone_id           0\n",
       "date_time         0\n",
       "load          30240\n",
       "station_1      3360\n",
       "station_2      3360\n",
       "station_3      3360\n",
       "station_4      3360\n",
       "station_5      3360\n",
       "station_6      3360\n",
       "station_7      3360\n",
       "station_8      3360\n",
       "station_9      3360\n",
       "station_10     3360\n",
       "station_11     3360\n",
       "dtype: int64"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_all.isnull().sum() #check for missing values after the merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "6c7efa26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        zone_id           date_time     load  station_1  station_2  station_3  \\\n",
      "0             1 2004-01-01 00:00:00  16853.0       46.0       38.0       44.0   \n",
      "1             1 2004-01-01 01:00:00  16450.0       46.0       36.0       42.0   \n",
      "2             1 2004-01-01 02:00:00  16517.0       45.0       35.0       40.0   \n",
      "3             1 2004-01-01 03:00:00  16873.0       41.0       30.0       36.0   \n",
      "4             1 2004-01-01 04:00:00  17064.0       39.0       30.0       34.0   \n",
      "...         ...                 ...      ...        ...        ...        ...   \n",
      "761755       20 2008-06-30 19:00:00  74728.0       67.0       54.0       63.5   \n",
      "761756       20 2008-06-30 20:00:00  74728.0       65.5       51.0       61.5   \n",
      "761757       20 2008-06-30 21:00:00  74728.0       64.5       48.5       58.0   \n",
      "761758       20 2008-06-30 22:00:00  74728.0       61.0       48.5       59.0   \n",
      "761759       20 2008-06-30 23:00:00  74728.0       60.0       48.5       59.0   \n",
      "\n",
      "        station_4  station_5  station_6  station_7  station_8  station_9  \\\n",
      "0            45.0       42.0       44.0       45.0       43.0       41.0   \n",
      "1            43.0       42.0       43.0       44.0       44.0       39.0   \n",
      "2            41.0       40.0       42.0       41.0       42.0       36.0   \n",
      "3            37.0       39.0       38.0       40.0       34.0       35.0   \n",
      "4            33.0       40.0       38.0       35.0       30.0       33.0   \n",
      "...           ...        ...        ...        ...        ...        ...   \n",
      "761755       64.0       65.0       57.5       61.0       63.0       62.5   \n",
      "761756       56.5       64.0       56.0       57.5       55.5       62.5   \n",
      "761757       54.0       62.5       55.0       56.5       55.5       60.0   \n",
      "761758       51.0       61.0       56.0       59.0       55.0       60.0   \n",
      "761759       52.0       60.0       56.0       59.5       55.0       61.0   \n",
      "\n",
      "        station_10  station_11  \n",
      "0             42.0        36.0  \n",
      "1             43.0        32.0  \n",
      "2             43.0        31.0  \n",
      "3             39.0        30.0  \n",
      "4             35.0        34.0  \n",
      "...            ...         ...  \n",
      "761755        63.0        78.0  \n",
      "761756        60.0        73.0  \n",
      "761757        59.5        72.0  \n",
      "761758        58.0        70.0  \n",
      "761759        56.0        70.0  \n",
      "\n",
      "[761760 rows x 14 columns]\n"
     ]
    }
   ],
   "source": [
    "merged_all_cleaned = merged_all.dropna().reset_index(drop=True) #drop rows with any NaNs and reset index\n",
    "print(merged_all_cleaned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "f4c911f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "zone_id       0\n",
       "date_time     0\n",
       "load          0\n",
       "station_1     0\n",
       "station_2     0\n",
       "station_3     0\n",
       "station_4     0\n",
       "station_5     0\n",
       "station_6     0\n",
       "station_7     0\n",
       "station_8     0\n",
       "station_9     0\n",
       "station_10    0\n",
       "station_11    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_all_cleaned.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eed306b",
   "metadata": {},
   "source": [
    "## Build time features:\n",
    "####    •\tcyclical encodings (hour/dow/doy)\n",
    "####\t•\tweekend + US holiday flags (holiday, day-before, day-after)\n",
    "####\t•\tapproximate sunrise/sunset + daylight flags (no lat/lon as the geografical zone is unknown) \n",
    "####\t•\tHeating Degree Days(HDD) / Cooling Degree Days (CDD) from all temp_station_* columns (auto-detects temps) with a threshold of 18 °C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "b699b9f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "                            Unnamed: 0                 2004  \\\n",
      "0                       New Year's Day  Thursday, January 1   \n",
      "1  Birthday of Martin Luther King, Jr.   Monday, January 19   \n",
      "2                Washington's Birthday  Monday, February 16   \n",
      "3                         Memorial Day       Monday, May 31   \n",
      "4                     Independence Day       Monday, July 5   \n",
      "\n",
      "                        2005                 2006                 2007  \\\n",
      "0  Friday, December 31, 2004    Monday, January 2    Monday, January 1   \n",
      "1         Monday, January 17   Monday, January 16   Monday, January 15   \n",
      "2        Monday, February 21  Monday, February 20  Monday, February 19   \n",
      "3             Monday, May 30       Monday, May 29       Monday, May 28   \n",
      "4             Monday, July 4      Tuesday, July 4    Wednesday, July 4   \n",
      "\n",
      "                  2008  \n",
      "0   Tuesday, January 1  \n",
      "1   Monday, January 21  \n",
      "2  Monday, February 18  \n",
      "3       Monday, May 26  \n",
      "4       Friday, July 4  \n"
     ]
    }
   ],
   "source": [
    "holidays = pd.read_csv(base_dir / 'data' / 'Load' / 'Holiday_List.csv')\n",
    "print(type(holidays))\n",
    "print(holidays.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "c81af8d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   zone_id           date_time     load  station_1  station_2  station_3  \\\n",
      "0        1 2004-01-01 00:00:00  16853.0       46.0       38.0       44.0   \n",
      "1        1 2004-01-01 01:00:00  16450.0       46.0       36.0       42.0   \n",
      "2        1 2004-01-01 02:00:00  16517.0       45.0       35.0       40.0   \n",
      "3        1 2004-01-01 03:00:00  16873.0       41.0       30.0       36.0   \n",
      "4        1 2004-01-01 04:00:00  17064.0       39.0       30.0       34.0   \n",
      "\n",
      "   station_4  station_5  station_6  station_7  ...  is_day_before_holiday  \\\n",
      "0       45.0       42.0       44.0       45.0  ...                      0   \n",
      "1       43.0       42.0       43.0       44.0  ...                      0   \n",
      "2       41.0       40.0       42.0       41.0  ...                      0   \n",
      "3       37.0       39.0       38.0       40.0  ...                      0   \n",
      "4       33.0       40.0       38.0       35.0  ...                      0   \n",
      "\n",
      "   is_day_after_holiday  sunrise_hour_approx  sunset_hour_approx  \\\n",
      "0                     0             5.000221           20.499705   \n",
      "1                     0             5.000221           20.499705   \n",
      "2                     0             5.000221           20.499705   \n",
      "3                     0             5.000221           20.499705   \n",
      "4                     0             5.000221           20.499705   \n",
      "\n",
      "   daylight_hours_approx  is_daylight_approx  daylight_proxy  temp_mean  HDD  \\\n",
      "0              15.499484                   0        0.017166  42.363636  0.0   \n",
      "1              15.499484                   0        0.017166  41.272727  0.0   \n",
      "2              15.499484                   0        0.017166  39.636364  0.0   \n",
      "3              15.499484                   0        0.017166  36.272727  0.0   \n",
      "4              15.499484                   0        0.017166  34.636364  0.0   \n",
      "\n",
      "         CDD  \n",
      "0  24.363636  \n",
      "1  23.272727  \n",
      "2  21.636364  \n",
      "3  18.272727  \n",
      "4  16.636364  \n",
      "\n",
      "[5 rows x 34 columns]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# --- Robust: convert wide holiday matrix (rows=holiday names, cols=years) into tidy 'date' list\n",
    "def _normalize_holiday_matrix(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Handles cases where the holiday name is in the index or in the first column,\n",
    "    and drops any non-year columns like 'Unnamed: 0' before parsing.\n",
    "    \"\"\"\n",
    "    # If holiday names are the index, promote to a column named 'holiday'\n",
    "    if df.index.name is None or df.index.equals(pd.RangeIndex(len(df))):\n",
    "        # Try to detect when the first column is the holiday name\n",
    "        first_col = df.columns[0]\n",
    "        # If the first column looks like year (numeric), then names must be in the index -> add from index\n",
    "        if pd.to_numeric(pd.Index(df.columns), errors=\"coerce\").notna().sum() > 0 and not pd.api.types.is_object_dtype(df[first_col]):\n",
    "            df2 = df.rename_axis(\"holiday\").reset_index()\n",
    "        else:\n",
    "            # Assume first column holds holiday names\n",
    "            df2 = df.copy()\n",
    "            if first_col.lower() != \"holiday\":\n",
    "                df2 = df2.rename(columns={first_col: \"holiday\"})\n",
    "    else:\n",
    "        df2 = df.rename_axis(\"holiday\").reset_index()\n",
    "\n",
    "    # Melt to long format\n",
    "    long = df2.melt(id_vars=[\"holiday\"], var_name=\"year\", value_name=\"raw\")\n",
    "\n",
    "    # Keep only rows where 'year' is numeric (drop 'Unnamed: 0' etc.)\n",
    "    long[\"year_num\"] = pd.to_numeric(long[\"year\"], errors=\"coerce\")\n",
    "    long = long[long[\"year_num\"].notna()].copy()\n",
    "\n",
    "    # Clean raw strings; drop blanks\n",
    "    long[\"raw\"] = long[\"raw\"].astype(str).str.strip()\n",
    "    long = long[~long[\"raw\"].isin([\"\", \"nan\", \"NaN\"])]\n",
    "\n",
    "    # Parse each cell: if text already contains a 4-digit year, parse as-is; else append the numeric year\n",
    "    def _parse_row(row):\n",
    "        txt = row[\"raw\"].strip().strip('\"').strip(\"'\")\n",
    "        yr  = int(row[\"year_num\"])\n",
    "        txt = re.sub(r\"\\s+,\", \",\", txt)          # fix stray spaces before commas\n",
    "        txt = re.sub(r\"\\s{2,}\", \" \", txt)        # collapse multiple spaces\n",
    "        if re.search(r\"\\b\\d{4}\\b\", txt):\n",
    "            dt = pd.to_datetime(txt, errors=\"coerce\")\n",
    "        else:\n",
    "            dt = pd.to_datetime(f\"{txt}, {yr}\", errors=\"coerce\")\n",
    "        return dt\n",
    "\n",
    "    long[\"date\"] = long.apply(_parse_row, axis=1)\n",
    "    dates = (long[\"date\"]\n",
    "             .dropna()\n",
    "             .dt.normalize()\n",
    "             .drop_duplicates()\n",
    "             .sort_values())\n",
    "    return pd.DataFrame({\"date\": dates})\n",
    "\n",
    "\n",
    "def make_features(\n",
    "    df: pd.DataFrame,\n",
    "    holidays,                     # can be a path or a wide-format DataFrame\n",
    "    *,\n",
    "    date_col: str = \"date_time\",\n",
    "    temp_prefix: str = \"temp_station_\",\n",
    "    base_c: float = 18.0,       # base temperature for HDD/CDD calculations (°C)\n",
    "    doy_period: int = 366,      # leap-year friendly\n",
    "    sr_mean: float = 6.5,       # 06:30 average sunrise time\n",
    "    sr_amp: float = 1.5,        # +/- 1.5 hours amplitude (earliest sunrise ~05:00, latest ~08:00) \n",
    "    ss_mean: float = 18.5,      # 18:30 average sunset time\n",
    "    ss_amp: float = 2.0         # +/- 2.0 hours amplitude (earliest sunset ~17:30, latest ~20:30)\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Adds:\n",
    "      • cyclical encodings (hour/dow/doy)\n",
    "      • weekend + holiday flags\n",
    "      • approximate sunrise/sunset + daylight\n",
    "      • HDD/CDD from all temp_station_* columns (°C)\n",
    "\n",
    "    Parameters:\n",
    "    - df: Input dataframe with a datetime column and temperature columns\n",
    "    - holidays: DataFrame containing US holidays with a 'date' column\n",
    "    - date_col: Name of the datetime column in df\n",
    "    - temp_prefix: Prefix for temperature columns to consider for HDD/CDD calculations\n",
    "    - base_c: Base temperature in Celsius for HDD/CDD calculations\n",
    "    - doy_period: Period for cyclical encoding of day of year (366 to include leap year)\n",
    "    - sr_mean, sr_amp: Mean and amplitude for sunrise time approximation\n",
    "    - ss_mean, ss_amp: Mean and amplitude for sunset time approximation\n",
    "    \"\"\"\n",
    "    out = df.copy()\n",
    "\n",
    "    # ---- timestamp\n",
    "    out[date_col] = pd.to_datetime(out[date_col], errors=\"coerce\")\n",
    "    if out[date_col].isna().any():\n",
    "        raise ValueError(f\"{date_col} contains NaT after parsing.\")\n",
    "    dt = out[date_col]\n",
    "\n",
    "    out[\"hour\"] = dt.dt.hour\n",
    "    out[\"dow\"] = dt.dt.dayofweek\n",
    "    out[\"is_weekend\"] = (out[\"dow\"] >= 5).astype(int)\n",
    "    doy = dt.dt.dayofyear.astype(int)\n",
    "\n",
    "    # ---- cyclical encodings\n",
    "    out[\"hour_sin\"] = np.sin(2 * pi * out[\"hour\"] / 24)\n",
    "    out[\"hour_cos\"] = np.cos(2 * pi * out[\"hour\"] / 24)\n",
    "    out[\"dow_sin\"]  = np.sin(2 * pi * out[\"dow\"] / 7)\n",
    "    out[\"dow_cos\"]  = np.cos(2 * pi * out[\"dow\"] / 7)\n",
    "    out[\"doy_sin\"]  = np.sin(2 * pi * doy / doy_period)\n",
    "    out[\"doy_cos\"]  = np.cos(2 * pi * doy / doy_period)\n",
    "\n",
    "    # ---- holidays (accept path or DataFrame)\n",
    "    if isinstance(holidays, (str, os.PathLike)):\n",
    "        holidays_df = pd.read_csv(holidays)\n",
    "    elif isinstance(holidays, pd.DataFrame):\n",
    "        holidays_df = holidays.copy()\n",
    "    else:\n",
    "        raise ValueError(\"holidays must be a CSV path or a DataFrame\")\n",
    "\n",
    "    # normalize the wide holiday matrix into a single 'date' column\n",
    "    holidays_norm = _normalize_holiday_matrix(holidays_df)\n",
    "    hol_days = set(holidays_norm[\"date\"].unique())\n",
    "    cal_day = dt.dt.normalize()\n",
    "\n",
    "    out[\"is_holiday\"]            = cal_day.isin(hol_days).astype(int)\n",
    "    out[\"is_day_before_holiday\"] = cal_day.isin({d - pd.Timedelta(days=1) for d in hol_days}).astype(int)\n",
    "    out[\"is_day_after_holiday\"]  = cal_day.isin({d + pd.Timedelta(days=1) for d in hol_days}).astype(int)\n",
    "\n",
    "    # sunrise/sunset-like approximation\n",
    "    sunrise_hour = sr_mean - sr_amp * np.cos(2 * pi * doy / doy_period)\n",
    "    sunset_hour  = ss_mean + ss_amp * np.cos(2 * pi * doy / doy_period)\n",
    "\n",
    "    out[\"sunrise_hour_approx\"]   = sunrise_hour\n",
    "    out[\"sunset_hour_approx\"]    = sunset_hour\n",
    "    out[\"daylight_hours_approx\"] = sunset_hour - sunrise_hour\n",
    "    out[\"is_daylight_approx\"]    = ((out[\"hour\"] >= sunrise_hour.astype(int)) &\n",
    "                                    (out[\"hour\"] <  sunset_hour.astype(int))).astype(int)\n",
    "    out[\"daylight_proxy\"]        = np.sin(2 * pi * doy / doy_period)\n",
    "\n",
    "    # ---- HDD/CDD from temperature columns (°C)\n",
    "    temp_cols = [c for c in out.columns if c.startswith(temp_prefix)]\n",
    "    if not temp_cols:\n",
    "        # fallback: any column containing 'temp'\n",
    "        temp_cols = [c for c in out.columns if re.search(r\"temp\", c, re.IGNORECASE)]\n",
    "    if temp_cols:\n",
    "        out[\"temp_mean\"] = out[temp_cols].mean(axis=1)\n",
    "        out[\"HDD\"] = (base_c - out[\"temp_mean\"]).clip(lower=0)\n",
    "        out[\"CDD\"] = (out[\"temp_mean\"] - base_c).clip(lower=0)\n",
    "\n",
    "    return out\n",
    "\n",
    "features_df = make_features(merged_all_cleaned, holidays, temp_prefix='station_')\n",
    "print(features_df.head())     "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e58d0832",
   "metadata": {},
   "source": [
    "## Create lag/rolling features\n",
    "#### The lag/rolling features are created only for exogenous variables -columns that are not the target(target is load) and are not identifiers (zone_id, date_time)- used later in an ARIMAX model (ARIMA with exogenous regressors). \n",
    "##### Examples: temperature (station_1,..., station_11, temp_mean), derived weather indicators (HDD, CDD), holidays(is_day_before_holiday, is_day_after_holiday), sunlight (sunrise_hour_approx, sunset_hour_approx, daylight_hours_approx, is_daylight_approx, daylight_proxy)\n",
    "##### Electricity load is strongly influenced by weather and holidays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "df5ad649",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def add_exogenous_lags_for_features_df(\n",
    "    df: pd.DataFrame,\n",
    "    *,\n",
    "    group_col: str = \"zone_id\",\n",
    "    date_col: str = \"date_time\",\n",
    "    # temperature-like columns\n",
    "    temp_prefix: str = \"station_\",                              # station_1..station_11\n",
    "    include_extra_temp_cols: Iterable[str] = (\"temp_mean\", \"HDD\", \"CDD\"),\n",
    "    temp_lags: tuple[int, ...] = (1, 3, 6, 24, 48, 168),        # lags in hours\n",
    "    temp_rolls: tuple[int, ...] = (6, 24, 168),                 # rolling windows in hours\n",
    "    # holiday columns present in features_df\n",
    "    holiday_cols: Iterable[str] = (\"is_day_before_holiday\", \"is_day_after_holiday\"),\n",
    "    holiday_lags: tuple[int, ...] = (),                         # usually not needed\n",
    "    holiday_leads: tuple[int, ...] = (24, 168)                  # usually only 1-day lead\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Adds lagged and rolling-window features for exogenous variables (non-target, non-ID columns)\n",
    "    to be used in an ARIMAX model (ARIMA with exogenous regressors).\n",
    "\n",
    "    Parameters:\n",
    "    - df: Input dataframe with a datetime column and temperature columns\n",
    "    - group_col: Column to group by (e.g., zone_id) for separate lagging per group\n",
    "    - date_col: Name of the datetime column in df\n",
    "    - temp_prefix: Prefix for temperature columns to consider for lagging/rolling\n",
    "    - include_extra_temp_cols: Additional temperature-like columns to include (e.g., temp_mean, HDD, CDD)\n",
    "    - temp_lags: Tuple of integer lags (in hours) to create for temperature-like columns\n",
    "    - temp_rolls: Tuple of integer rolling window sizes (in hours) to create for temperature-like columns\n",
    "    - holidays_cols: List of holiday-related binary columns to consider for lagging/leading\n",
    "    - holiday_lags: Tuple of integer lags (in hours) to create for holiday columns\n",
    "    - holiday_leads: Tuple of integer leads (in hours) to create for holiday columns\n",
    "\n",
    "    Returns:\n",
    "    - DataFrame with original and new lagged/rolling features\n",
    "    \"\"\"\n",
    "    out = df.copy()\n",
    "    out = out.sort_values([group_col, date_col], kind=\"mergesort\")\n",
    "    out[date_col] = pd.to_datetime(out[date_col], errors=\"coerce\")\n",
    "\n",
    "    # ---- collect temperature columns\n",
    "    temp_cols = [c for c in out.columns if c.startswith(temp_prefix)]\n",
    "    for c in include_extra_temp_cols:\n",
    "        if c in out.columns:\n",
    "            temp_cols.append(c)\n",
    "    temp_cols = list(dict.fromkeys(temp_cols))  # de-duplicate while keeping order\n",
    "\n",
    "    if not temp_cols:\n",
    "        raise ValueError(\"No temperature columns found (station_* / temp_mean / HDD / CDD).\")\n",
    "\n",
    "    # ---- group per zone\n",
    "    g = out.groupby(group_col, group_keys=False)\n",
    "\n",
    "    # ---- temperature lags\n",
    "    for L in temp_lags:\n",
    "        for c in temp_cols:\n",
    "            out[f\"{c}_lag{L}\"] = g[c].shift(L)\n",
    "\n",
    "    # ---- temperature rolling means\n",
    "    for W in temp_rolls:\n",
    "        for c in temp_cols:\n",
    "            out[f\"{c}_roll{W}\"] = g[c].transform(lambda s: s.rolling(W, min_periods=1).mean())\n",
    "\n",
    "    # ---- holiday lags/leads\n",
    "    hol_cols_present = [h for h in holiday_cols if h in out.columns]\n",
    "\n",
    "    for L in holiday_lags:\n",
    "        for h in hol_cols_present:\n",
    "            out[f\"{h}_lag{L}\"] = g[h].shift(L)\n",
    "\n",
    "    for H in holiday_leads:\n",
    "        for h in hol_cols_present:\n",
    "            out[f\"{h}_lead{H}\"] = g[h].shift(-H)\n",
    "\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "8cf4e473",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/y6/w4x154ys7zd4n1hm8b7sn2hh0000gp/T/ipykernel_57161/1059531882.py:60: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  out[f\"{c}_roll{W}\"] = g[c].transform(lambda s: s.rolling(W, min_periods=1).mean())\n",
      "/var/folders/y6/w4x154ys7zd4n1hm8b7sn2hh0000gp/T/ipykernel_57161/1059531882.py:60: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  out[f\"{c}_roll{W}\"] = g[c].transform(lambda s: s.rolling(W, min_periods=1).mean())\n",
      "/var/folders/y6/w4x154ys7zd4n1hm8b7sn2hh0000gp/T/ipykernel_57161/1059531882.py:60: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  out[f\"{c}_roll{W}\"] = g[c].transform(lambda s: s.rolling(W, min_periods=1).mean())\n",
      "/var/folders/y6/w4x154ys7zd4n1hm8b7sn2hh0000gp/T/ipykernel_57161/1059531882.py:60: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  out[f\"{c}_roll{W}\"] = g[c].transform(lambda s: s.rolling(W, min_periods=1).mean())\n",
      "/var/folders/y6/w4x154ys7zd4n1hm8b7sn2hh0000gp/T/ipykernel_57161/1059531882.py:60: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  out[f\"{c}_roll{W}\"] = g[c].transform(lambda s: s.rolling(W, min_periods=1).mean())\n",
      "/var/folders/y6/w4x154ys7zd4n1hm8b7sn2hh0000gp/T/ipykernel_57161/1059531882.py:60: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  out[f\"{c}_roll{W}\"] = g[c].transform(lambda s: s.rolling(W, min_periods=1).mean())\n",
      "/var/folders/y6/w4x154ys7zd4n1hm8b7sn2hh0000gp/T/ipykernel_57161/1059531882.py:60: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  out[f\"{c}_roll{W}\"] = g[c].transform(lambda s: s.rolling(W, min_periods=1).mean())\n",
      "/var/folders/y6/w4x154ys7zd4n1hm8b7sn2hh0000gp/T/ipykernel_57161/1059531882.py:60: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  out[f\"{c}_roll{W}\"] = g[c].transform(lambda s: s.rolling(W, min_periods=1).mean())\n",
      "/var/folders/y6/w4x154ys7zd4n1hm8b7sn2hh0000gp/T/ipykernel_57161/1059531882.py:60: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  out[f\"{c}_roll{W}\"] = g[c].transform(lambda s: s.rolling(W, min_periods=1).mean())\n",
      "/var/folders/y6/w4x154ys7zd4n1hm8b7sn2hh0000gp/T/ipykernel_57161/1059531882.py:60: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  out[f\"{c}_roll{W}\"] = g[c].transform(lambda s: s.rolling(W, min_periods=1).mean())\n",
      "/var/folders/y6/w4x154ys7zd4n1hm8b7sn2hh0000gp/T/ipykernel_57161/1059531882.py:60: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  out[f\"{c}_roll{W}\"] = g[c].transform(lambda s: s.rolling(W, min_periods=1).mean())\n",
      "/var/folders/y6/w4x154ys7zd4n1hm8b7sn2hh0000gp/T/ipykernel_57161/1059531882.py:60: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  out[f\"{c}_roll{W}\"] = g[c].transform(lambda s: s.rolling(W, min_periods=1).mean())\n",
      "/var/folders/y6/w4x154ys7zd4n1hm8b7sn2hh0000gp/T/ipykernel_57161/1059531882.py:60: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  out[f\"{c}_roll{W}\"] = g[c].transform(lambda s: s.rolling(W, min_periods=1).mean())\n",
      "/var/folders/y6/w4x154ys7zd4n1hm8b7sn2hh0000gp/T/ipykernel_57161/1059531882.py:60: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  out[f\"{c}_roll{W}\"] = g[c].transform(lambda s: s.rolling(W, min_periods=1).mean())\n",
      "/var/folders/y6/w4x154ys7zd4n1hm8b7sn2hh0000gp/T/ipykernel_57161/1059531882.py:60: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  out[f\"{c}_roll{W}\"] = g[c].transform(lambda s: s.rolling(W, min_periods=1).mean())\n",
      "/var/folders/y6/w4x154ys7zd4n1hm8b7sn2hh0000gp/T/ipykernel_57161/1059531882.py:60: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  out[f\"{c}_roll{W}\"] = g[c].transform(lambda s: s.rolling(W, min_periods=1).mean())\n",
      "/var/folders/y6/w4x154ys7zd4n1hm8b7sn2hh0000gp/T/ipykernel_57161/1059531882.py:60: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  out[f\"{c}_roll{W}\"] = g[c].transform(lambda s: s.rolling(W, min_periods=1).mean())\n",
      "/var/folders/y6/w4x154ys7zd4n1hm8b7sn2hh0000gp/T/ipykernel_57161/1059531882.py:60: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  out[f\"{c}_roll{W}\"] = g[c].transform(lambda s: s.rolling(W, min_periods=1).mean())\n",
      "/var/folders/y6/w4x154ys7zd4n1hm8b7sn2hh0000gp/T/ipykernel_57161/1059531882.py:60: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  out[f\"{c}_roll{W}\"] = g[c].transform(lambda s: s.rolling(W, min_periods=1).mean())\n",
      "/var/folders/y6/w4x154ys7zd4n1hm8b7sn2hh0000gp/T/ipykernel_57161/1059531882.py:60: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  out[f\"{c}_roll{W}\"] = g[c].transform(lambda s: s.rolling(W, min_periods=1).mean())\n",
      "/var/folders/y6/w4x154ys7zd4n1hm8b7sn2hh0000gp/T/ipykernel_57161/1059531882.py:60: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  out[f\"{c}_roll{W}\"] = g[c].transform(lambda s: s.rolling(W, min_periods=1).mean())\n",
      "/var/folders/y6/w4x154ys7zd4n1hm8b7sn2hh0000gp/T/ipykernel_57161/1059531882.py:60: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  out[f\"{c}_roll{W}\"] = g[c].transform(lambda s: s.rolling(W, min_periods=1).mean())\n",
      "/var/folders/y6/w4x154ys7zd4n1hm8b7sn2hh0000gp/T/ipykernel_57161/1059531882.py:60: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  out[f\"{c}_roll{W}\"] = g[c].transform(lambda s: s.rolling(W, min_periods=1).mean())\n",
      "/var/folders/y6/w4x154ys7zd4n1hm8b7sn2hh0000gp/T/ipykernel_57161/1059531882.py:60: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  out[f\"{c}_roll{W}\"] = g[c].transform(lambda s: s.rolling(W, min_periods=1).mean())\n",
      "/var/folders/y6/w4x154ys7zd4n1hm8b7sn2hh0000gp/T/ipykernel_57161/1059531882.py:60: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  out[f\"{c}_roll{W}\"] = g[c].transform(lambda s: s.rolling(W, min_periods=1).mean())\n",
      "/var/folders/y6/w4x154ys7zd4n1hm8b7sn2hh0000gp/T/ipykernel_57161/1059531882.py:60: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  out[f\"{c}_roll{W}\"] = g[c].transform(lambda s: s.rolling(W, min_periods=1).mean())\n",
      "/var/folders/y6/w4x154ys7zd4n1hm8b7sn2hh0000gp/T/ipykernel_57161/1059531882.py:60: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  out[f\"{c}_roll{W}\"] = g[c].transform(lambda s: s.rolling(W, min_periods=1).mean())\n",
      "/var/folders/y6/w4x154ys7zd4n1hm8b7sn2hh0000gp/T/ipykernel_57161/1059531882.py:60: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  out[f\"{c}_roll{W}\"] = g[c].transform(lambda s: s.rolling(W, min_periods=1).mean())\n",
      "/var/folders/y6/w4x154ys7zd4n1hm8b7sn2hh0000gp/T/ipykernel_57161/1059531882.py:60: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  out[f\"{c}_roll{W}\"] = g[c].transform(lambda s: s.rolling(W, min_periods=1).mean())\n",
      "/var/folders/y6/w4x154ys7zd4n1hm8b7sn2hh0000gp/T/ipykernel_57161/1059531882.py:60: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  out[f\"{c}_roll{W}\"] = g[c].transform(lambda s: s.rolling(W, min_periods=1).mean())\n",
      "/var/folders/y6/w4x154ys7zd4n1hm8b7sn2hh0000gp/T/ipykernel_57161/1059531882.py:71: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  out[f\"{h}_lead{H}\"] = g[h].shift(-H)\n",
      "/var/folders/y6/w4x154ys7zd4n1hm8b7sn2hh0000gp/T/ipykernel_57161/1059531882.py:71: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  out[f\"{h}_lead{H}\"] = g[h].shift(-H)\n",
      "/var/folders/y6/w4x154ys7zd4n1hm8b7sn2hh0000gp/T/ipykernel_57161/1059531882.py:71: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  out[f\"{h}_lead{H}\"] = g[h].shift(-H)\n",
      "/var/folders/y6/w4x154ys7zd4n1hm8b7sn2hh0000gp/T/ipykernel_57161/1059531882.py:71: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  out[f\"{h}_lead{H}\"] = g[h].shift(-H)\n"
     ]
    }
   ],
   "source": [
    "features_with_lags = add_exogenous_lags_for_features_df(\n",
    "    features_df,                    # my DataFrame\n",
    "    group_col=\"zone_id\",\n",
    "    date_col=\"date_time\",\n",
    "    temp_prefix=\"station_\",         # station_1..station_11\n",
    "    include_extra_temp_cols=(\"temp_mean\", \"HDD\", \"CDD\"),\n",
    "    temp_lags=(1, 3, 6, 24, 48, 168),\n",
    "    temp_rolls=(6, 24, 168),\n",
    "    holiday_cols=(\"is_day_before_holiday\", \"is_day_after_holiday\"),\n",
    "    holiday_lags=(),                # usually leave empty\n",
    "    holiday_leads=(24, 168)         # holidays are known → safe leads\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6db6533c",
   "metadata": {},
   "source": [
    "## Plot the obtained data frame with time features (features_df) to visualize trends, seasonality and holiday effects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e5483f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at loaad demand for a specific zone over the full timeline\n",
    "feat = features_df.copy()\n",
    "zone = feat['zone_id'].unique()[0]  # pick first zone\n",
    "df_zone = feat[feat['zone_id'] == zone].set_index('date_time')\n",
    "\n",
    "plt.figure(figsize=(15,5))\n",
    "plt.plot(df_zone['load'], label=f\"Zone {zone}\")\n",
    "plt.title(\"Load over time\")\n",
    "plt.ylabel(\"Megawatts (MW)\")\n",
    "plt.xlabel(\"Date\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce1769b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Average load by hour and day of week\n",
    "# Hourly pattern\n",
    "sns.lineplot(x=\"hour\", y=\"load\", data=feat, estimator=\"mean\", errorbar=None)\n",
    "plt.title(\"Average daily load profile\")\n",
    "plt.xlabel(\"Hour of Day\")\n",
    "plt.ylabel(\"Average Load\")\n",
    "plt.show()\n",
    "\n",
    "# Weekly pattern\n",
    "sns.lineplot(x=\"dow\", y=\"load\", data=feat, estimator=\"mean\", errorbar=None)\n",
    "plt.title(\"Average load by day of week (0=Mon)\")\n",
    "plt.xlabel(\"Day of Week (0=Mon, 6=Sun)\")\n",
    "plt.ylabel(\"Average Load\")\n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee1ac61f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Average load by month and day of year\n",
    "# Monthly pattern\n",
    "feat['month'] = feat['date_time'].dt.month\n",
    "sns.lineplot(x=\"month\", y=\"load\", data=feat, estimator=\"mean\", errorbar=None)\n",
    "plt.title(\"Average monthly load\")\n",
    "plt.show()\n",
    "\n",
    "# Smoother annual seasonality\n",
    "feat['doy'] = feat['date_time'].dt.dayofyear\n",
    "sns.lineplot(x=\"doy\", y=\"load\", data=feat, estimator=\"mean\", errorbar=None)\n",
    "plt.title(\"Average load by day of year\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "127ccfa9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "explainable_ai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
